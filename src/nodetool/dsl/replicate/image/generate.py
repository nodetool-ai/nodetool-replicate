# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class AdInpaint(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Product advertising image generator
    """

    Pixel: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.AdInpaint.Pixel
    )
    Product_size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.AdInpaint.Product_size
    )

    pixel: nodetool.nodes.replicate.image.generate.AdInpaint.Pixel = Field(
        default=nodetool.nodes.replicate.image.generate.AdInpaint.Pixel("512 * 512"),
        description="image total pixel",
    )
    scale: int | OutputHandle[int] = connect_field(
        default=3, description="Factor to scale image by (maximum: 4)"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Product name or prompt"
    )
    image_num: int | OutputHandle[int] = connect_field(
        default=1, description="Number of image to generate"
    )
    image_path: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="input image",
    )
    manual_seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Manual Seed"
    )
    product_size: nodetool.nodes.replicate.image.generate.AdInpaint.Product_size = (
        Field(
            default=nodetool.nodes.replicate.image.generate.AdInpaint.Product_size(
                "Original"
            ),
            description="Max product size",
        )
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance Scale"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, out of frame, illustration, 3d, sepia, painting, cartoons, sketch, watermark, text, Logo, advertisement",
        description="Anything you don't want in the photo",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=20, description="Inference Steps"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.AdInpaint

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class ConsistentCharacter(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    Create images of a given character in different poses
    """

    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.ConsistentCharacter.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Set a seed for reproducibility. Random by default."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A headshot photo",
        description="Describe the subject. Include clothes and hairstyle for more consistency.",
    )
    subject: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image of a person. Best images are square close ups of a face, but they do not have to be.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.ConsistentCharacter.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.ConsistentCharacter.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality of the output images, from 0 to 100. 100 is best quality, 0 is lowest quality.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Things you do not want to see in your image"
    )
    randomise_poses: bool | OutputHandle[bool] = connect_field(
        default=True, description="Randomise the poses used."
    )
    number_of_outputs: int | OutputHandle[int] = connect_field(
        default=3, description="The number of images to generate."
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )
    number_of_images_per_pose: int | OutputHandle[int] = connect_field(
        default=1, description="The number of images to generate for each pose."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.ConsistentCharacter

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_1_1_Pro_Ultra(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    FLUX1.1 [pro] in ultra and raw modes. Images are up to 4 megapixels. Use raw mode for realism.
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_1_1_Pro_Ultra.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_1_1_Pro_Ultra.Output_format
    )

    raw: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Generate less processed, more natural-looking images",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_1_1_Pro_Ultra.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_1_1_Pro_Ultra.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image",
    )
    image_prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Image to use with Flux Redux. This is used together with the text prompt to guide the generation towards the composition of the image_prompt. Must be jpeg, png, gif, or webp.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_1_1_Pro_Ultra.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_1_1_Pro_Ultra.Output_format(
            "jpg"
        ),
        description="Format of the output images.",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 6 is most permissive",
    )
    image_prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.1, description="Blend between the prompt and the image prompt."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_1_1_Pro_Ultra

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_2_Flex(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Max-quality image generation and editing with support for ten reference images
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Flex.Resolution
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Flex.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Flex.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps"
    )
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of the generated image. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32).",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of the generated image. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32).",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=4.5,
        description="Guidance scale for generation. Controls how closely the output follows the prompt",
    )
    resolution: nodetool.nodes.replicate.image.generate.Flux_2_Flex.Resolution = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_2_Flex.Resolution("1 MP"),
        description="Resolution in megapixels. Up to 4 MP is possible, but 2 MP or below is recommended. The maximum image size is 2048x2048, which means that high-resolution images may not respect the resolution if aspect ratio is not 1:1.\n\nResolution is not used when aspect_ratio is 'custom'. When aspect_ratio is 'match_input_image', use 'match_input_image' to match the input image's resolution (clamped to 0.5-4 MP).",
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_2_Flex.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_2_Flex.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio for the generated image. Use 'match_input_image' to match the first input image's aspect ratio.",
        )
    )
    input_images: list | OutputHandle[list] = connect_field(
        default=[],
        description="List of input images for image-to-image generation. Maximum 10 images. Must be jpeg, png, gif, or webp.",
    )
    output_format: nodetool.nodes.replicate.image.generate.Flux_2_Flex.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_2_Flex.Output_format(
                "webp"
            ),
            description="Format of the output images.",
        )
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 5 is most permissive",
    )
    prompt_upsampling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Automatically modify the prompt for more creative generation",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_2_Flex

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_2_Max(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    The highest fidelity image model from Black Forest Labs
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Max.Resolution
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Max.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Max.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of the generated image. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32).",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of the generated image. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32).",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    resolution: nodetool.nodes.replicate.image.generate.Flux_2_Max.Resolution = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_2_Max.Resolution("1 MP"),
        description="Resolution in megapixels. Up to 4 MP is possible, but 2 MP or below is recommended. The maximum image size is 2048x2048, which means that high-resolution images may not respect the resolution if aspect ratio is not 1:1.\n\nResolution is not used when aspect_ratio is 'custom'. When aspect_ratio is 'match_input_image', use 'match_input_image' to match the input image's resolution (clamped to 0.5-4 MP).",
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_2_Max.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_2_Max.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio for the generated image. Use 'match_input_image' to match the first input image's aspect ratio.",
        )
    )
    input_images: list | OutputHandle[list] = connect_field(
        default=[],
        description="List of input images for image-to-image generation. Maximum 8 images. Must be jpeg, png, gif, or webp.",
    )
    output_format: nodetool.nodes.replicate.image.generate.Flux_2_Max.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_2_Max.Output_format(
                "webp"
            ),
            description="Format of the output images.",
        )
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 5 is most permissive",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_2_Max

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_2_Pro(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    High-quality image generation and editing with support for eight reference images
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Pro.Resolution
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Pro.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_2_Pro.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of the generated image. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32).",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of the generated image. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32).",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    resolution: nodetool.nodes.replicate.image.generate.Flux_2_Pro.Resolution = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_2_Pro.Resolution("1 MP"),
        description="Resolution in megapixels. Up to 4 MP is possible, but 2 MP or below is recommended. The maximum image size is 2048x2048, which means that high-resolution images may not respect the resolution if aspect ratio is not 1:1.\n\nResolution is not used when aspect_ratio is 'custom'. When aspect_ratio is 'match_input_image', use 'match_input_image' to match the input image's resolution (clamped to 0.5-4 MP).",
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_2_Pro.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_2_Pro.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio for the generated image. Use 'match_input_image' to match the first input image's aspect ratio.",
        )
    )
    input_images: list | OutputHandle[list] = connect_field(
        default=[],
        description="List of input images for image-to-image generation. Maximum 8 images. Must be jpeg, png, gif, or webp.",
    )
    output_format: nodetool.nodes.replicate.image.generate.Flux_2_Pro.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_2_Pro.Output_format(
                "webp"
            ),
            description="Format of the output images.",
        )
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 5 is most permissive",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_2_Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_360(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Generate 360 panorama images.
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_360.Model
    )
    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_360.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_360.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_360.Output_format
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image mask for image inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image or inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    model: nodetool.nodes.replicate.image.generate.Flux_360.Model = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_360.Model("dev"),
        description="Which model to run inference with. The dev model performs best with around 28 inference steps but the schnell model only needs 4 steps.",
    )
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Prompt for generated image. If you include the `trigger_word` used in the training process you are more likely to activate the trained object, style, or concept in the resulting image.",
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16",
    )
    extra_lora: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_360.Megapixels = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_360.Megapixels("1"),
        description="Approximate number of megapixels for generated image",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_360.Aspect_ratio = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_360.Aspect_ratio("1:1"),
        description="Aspect ratio for the generated image. If custom is selected, uses height and width below & will run in bf16 mode",
    )
    output_format: nodetool.nodes.replicate.image.generate.Flux_360.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_360.Output_format(
                "webp"
            ),
            description="Format of the output images",
        )
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3,
        description="Guidance scale for the diffusion process. Lower values can give more realistic images. Good values to try are 2, 2.5, 3 and 3.5",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img. 1.0 corresponds to full destruction of information in image",
    )
    extra_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the extra LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. More steps can give more detailed images, but take longer.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_360

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Black_Light(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    A flux lora fine-tuned on black light images
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Black_Light.Model
    )
    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Black_Light.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Black_Light.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Black_Light.Output_format
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image mask for image inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image or inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    model: nodetool.nodes.replicate.image.generate.Flux_Black_Light.Model = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Black_Light.Model("dev"),
        description="Which model to run inference with. The dev model performs best with around 28 inference steps but the schnell model only needs 4 steps.",
    )
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Prompt for generated image. If you include the `trigger_word` used in the training process you are more likely to activate the trained object, style, or concept in the resulting image.",
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16",
    )
    extra_lora: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Black_Light.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Black_Light.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_Black_Light.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Black_Light.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image. If custom is selected, uses height and width below & will run in bf16 mode",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Black_Light.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Black_Light.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3,
        description="Guidance scale for the diffusion process. Lower values can give more realistic images. Good values to try are 2, 2.5, 3 and 3.5",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img. 1.0 corresponds to full destruction of information in image",
    )
    extra_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the extra LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. More steps can give more detailed images, but take longer.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Black_Light

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Canny_Dev(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Open-weight edge-guided image generation. Control structure and composition using Canny edge detection.
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Canny_Dev.Megapixels
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Canny_Dev.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=30, description="Guidance for generated image"
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Canny_Dev.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Canny_Dev.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image. Use match_input to match the size of the input (with an upper limit of 1440x1440 pixels)",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image used to control the generation. The canny edge detection will be automatically generated.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Canny_Dev.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Canny_Dev.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. Recommended range is 28-50, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Canny_Dev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Canny_Pro(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Professional edge-guided image generation. Control structure and composition using Canny edge detection
    """

    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Canny_Pro.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Number of diffusion steps. Higher values yield finer details but increase processing time.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=30,
        description="Controls the balance between adherence to the text as well as image prompt and image quality/diversity. Higher values make the output more closely match the prompt but may reduce overall image quality. Lower values allow for more creative freedom but might produce results less relevant to the prompt.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as control input. Must be jpeg, png, gif, or webp.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Canny_Pro.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Canny_Pro.Output_format(
            "jpg"
        ),
        description="Format of the output images.",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 6 is most permissive",
    )
    prompt_upsampling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Automatically modify the prompt for more creative generation",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Canny_Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Cinestill(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Flux lora, use "CNSTLL" to trigger
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Cinestill.Model
    )
    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Cinestill.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Cinestill.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Cinestill.Output_format
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image mask for image inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image or inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    model: nodetool.nodes.replicate.image.generate.Flux_Cinestill.Model = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Cinestill.Model("dev"),
        description="Which model to run inference with. The dev model performs best with around 28 inference steps but the schnell model only needs 4 steps.",
    )
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Prompt for generated image. If you include the `trigger_word` used in the training process you are more likely to activate the trained object, style, or concept in the resulting image.",
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16",
    )
    extra_lora: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Cinestill.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Cinestill.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_Cinestill.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Cinestill.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image. If custom is selected, uses height and width below & will run in bf16 mode",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Cinestill.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Cinestill.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3,
        description="Guidance scale for the diffusion process. Lower values can give more realistic images. Good values to try are 2, 2.5, 3 and 3.5",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img. 1.0 corresponds to full destruction of information in image",
    )
    extra_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the extra LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. More steps can give more detailed images, but take longer.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Cinestill

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Depth_Dev(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Open-weight depth-aware image generation. Edit images while preserving spatial relationships.
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Depth_Dev.Megapixels
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Depth_Dev.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=10, description="Guidance for generated image"
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Depth_Dev.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Depth_Dev.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image. Use match_input to match the size of the input (with an upper limit of 1440x1440 pixels)",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image used to control the generation. The depth map will be automatically generated.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Depth_Dev.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Depth_Dev.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. Recommended range is 28-50, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Depth_Dev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Depth_Pro(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Professional depth-aware image generation. Edit images while preserving spatial relationships.
    """

    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Depth_Pro.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Number of diffusion steps. Higher values yield finer details but increase processing time.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=30,
        description="Controls the balance between adherence to the text as well as image prompt and image quality/diversity. Higher values make the output more closely match the prompt but may reduce overall image quality. Lower values allow for more creative freedom but might produce results less relevant to the prompt.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as control input. Must be jpeg, png, gif, or webp.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Depth_Pro.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Depth_Pro.Output_format(
            "jpg"
        ),
        description="Format of the output images.",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 6 is most permissive",
    )
    prompt_upsampling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Automatically modify the prompt for more creative generation",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Depth_Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Dev(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    A 12 billion parameter rectified flow transformer capable of generating images from text descriptions
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Dev.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Dev.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Dev.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Input image for image to image mode. The aspect ratio of your output will match this image",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16. Note that outputs will not be deterministic when this is enabled, even if you set a seed.",
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=3, description="Guidance for generated image"
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Dev.Megapixels = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Dev.Megapixels("1"),
        description="Approximate number of megapixels for generated image",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_Dev.Aspect_ratio = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Dev.Aspect_ratio("1:1"),
        description="Aspect ratio for the generated image",
    )
    output_format: nodetool.nodes.replicate.image.generate.Flux_Dev.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Dev.Output_format(
                "webp"
            ),
            description="Format of the output images",
        )
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img. 1.0 corresponds to full destruction of information in image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. Recommended range is 28-50, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Dev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Dev_Lora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    A version of flux-dev, a text to image model, that supports fast fine-tuned lora inference
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image mode. The aspect ratio of your output will match this image",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16. Note that outputs will not be deterministic when this is enabled, even if you set a seed.",
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=3, description="Guidance for generated image"
    )
    extra_lora: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio for the generated image",
        )
    )
    hf_api_token: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="HuggingFace API token. If you're using a hf lora that needs authentication, you'll need to provide an API token.",
    )
    lora_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>[/<lora-weights-file.safetensors>], CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet, including signed URLs. For example, 'fofr/flux-pixar-cars'. Civit AI and HuggingFace LoRAs may require an API token to access, which you can provide in the `civitai_api_token` and `hf_api_token` inputs respectively.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Dev_Lora.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img. 1.0 corresponds to full destruction of information in image",
    )
    extra_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the extra LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    civitai_api_token: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Civitai API token. If you're using a civitai lora that needs authentication, you'll need to provide an API token.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. Recommended range is 28-50, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Dev_Lora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Fill_Dev(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Open-weight inpainting model for editing and extending images. Guidance-distilled from FLUX.1 Fill [pro].
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Fill_Dev.Megapixels
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Fill_Dev.Output_format
    )

    mask: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="A black-and-white image that describes the part of the image to inpaint. Black areas will be preserved while white areas will be inpainted.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The image to inpaint. Can contain alpha mask. If the image width or height are not multiples of 32, they will be scaled to the closest multiple of 32. If the image dimensions don't fit within 1440x1440, it will be scaled down to fit.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=30, description="Guidance for generated image"
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Fill_Dev.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Fill_Dev.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image. Use match_input to match the size of the input (with an upper limit of 1440x1440 pixels)",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    lora_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Fill_Dev.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Fill_Dev.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. Recommended range is 28-50, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Fill_Dev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Fill_Pro(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Professional inpainting and outpainting model with state-of-the-art performance. Edit or extend images with natural, seamless results.
    """

    Outpaint: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Fill_Pro.Outpaint
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Fill_Pro.Output_format
    )

    mask: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="A black-and-white image that describes the part of the image to inpaint. Black areas will be preserved while white areas will be inpainted. Must have the same size as image. Optional if you provide an alpha mask in the original image. Must be jpeg, png, gif, or webp.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The image to inpaint. Can contain an alpha mask. Must be jpeg, png, gif, or webp.",
    )
    steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Number of diffusion steps. Higher values yield finer details but increase processing time.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=60,
        description="Controls the balance between adherence to the text prompt and image quality/diversity. Higher values make the output more closely match the prompt but may reduce overall image quality. Lower values allow for more creative freedom but might produce results less relevant to the prompt.",
    )
    outpaint: nodetool.nodes.replicate.image.generate.Flux_Fill_Pro.Outpaint = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Fill_Pro.Outpaint("None"),
        description="A quick option for outpainting an input image. Mask will be ignored.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Fill_Pro.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Fill_Pro.Output_format(
            "jpg"
        ),
        description="Format of the output images.",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 6 is most permissive",
    )
    prompt_upsampling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Automatically modify the prompt for more creative generation",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Fill_Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Kontext_Pro(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    A state-of-the-art text-based image editing model that delivers high-quality outputs with excellent prompt following and consistent results for transforming images through natural language
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Kontext_Pro.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Kontext_Pro.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Text description of what you want to generate, or the instruction on how to edit the given image.",
    )
    input_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Image to use as reference. Must be jpeg, png, gif, or webp.",
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_Kontext_Pro.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Kontext_Pro.Aspect_ratio(
            "match_input_image"
        ),
        description="Aspect ratio of the generated image. Use 'match_input_image' to match the aspect ratio of the input image.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Kontext_Pro.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Kontext_Pro.Output_format(
            "png"
        ),
        description="Output format for the generated image",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 0 is most strict and 6 is most permissive. 2 is currently the maximum allowed when input images are used.",
    )
    prompt_upsampling: bool | OutputHandle[bool] = connect_field(
        default=False, description="Automatic prompt improvement"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Kontext_Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Mona_Lisa(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Flux lora, use the term "MNALSA" to trigger generation
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Model
    )
    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Output_format
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image mask for image inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image or inpainting mode. If provided, aspect_ratio, width, and height inputs are ignored.",
    )
    model: nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Model = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Model("dev"),
        description="Which model to run inference with. The dev model performs best with around 28 inference steps but the schnell model only needs 4 steps.",
    )
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of generated image. Only works if `aspect_ratio` is set to custom. Will be rounded to nearest multiple of 16. Incompatible with fast generation",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Prompt for generated image. If you include the `trigger_word` used in the training process you are more likely to activate the trained object, style, or concept in the resulting image.",
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16",
    )
    extra_lora: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image. If custom is selected, uses height and width below & will run in bf16 mode",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3,
        description="Guidance scale for the diffusion process. Lower values can give more realistic images. Good values to try are 2, 2.5, 3 and 3.5",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img. 1.0 corresponds to full destruction of information in image",
    )
    extra_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the extra LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="Number of denoising steps. More steps can give more detailed images, but take longer.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Mona_Lisa

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Pro(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    State-of-the-art image generation with top of the line prompt following, visual quality, image detail and output diversity.
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Pro.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Pro.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    steps: int | OutputHandle[int] = connect_field(default=25, description="Deprecated")
    width: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Width of the generated image in text-to-image mode. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32). Note: Ignored in img2img and inpainting modes.",
    )
    height: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Height of the generated image in text-to-image mode. Only used when aspect_ratio=custom. Must be a multiple of 32 (if it's not, it will be rounded to nearest multiple of 32). Note: Ignored in img2img and inpainting modes.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=3,
        description="Controls the balance between adherence to the text prompt and image quality/diversity. Higher values make the output more closely match the prompt but may reduce overall image quality. Lower values allow for more creative freedom but might produce results less relevant to the prompt.",
    )
    interval: float | OutputHandle[float] = connect_field(
        default=2, description="Deprecated"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_Pro.Aspect_ratio = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Pro.Aspect_ratio("1:1"),
        description="Aspect ratio for the generated image",
    )
    image_prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Image to use with Flux Redux. This is used together with the text prompt to guide the generation towards the composition of the image_prompt. Must be jpeg, png, gif, or webp.",
    )
    output_format: nodetool.nodes.replicate.image.generate.Flux_Pro.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Pro.Output_format(
                "webp"
            ),
            description="Format of the output images.",
        )
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    safety_tolerance: int | OutputHandle[int] = connect_field(
        default=2,
        description="Safety tolerance, 1 is most strict and 6 is most permissive",
    )
    prompt_upsampling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Automatically modify the prompt for more creative generation",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Redux_Dev(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Open-weight image variation model. Create new versions while preserving key elements of your original.
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=3, description="Guidance for generated image"
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    redux_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to condition your output on. This replaces prompt for FLUX.1 Redux models",
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Redux_Dev.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="Number of denoising steps. Recommended range is 28-50"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Redux_Dev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Redux_Schnell(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    Fast, efficient image variation model for rapid iteration and experimentation.
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    megapixels: (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Megapixels
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Megapixels(
            "1"
        ),
        description="Approximate number of megapixels for generated image",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    redux_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to condition your output on. This replaces prompt for FLUX.1 Redux models",
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=4,
        description="Number of denoising steps. 4 is recommended, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Redux_Schnell

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Schnell(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    The fastest image generation model tailored for local development and personal use
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Schnell.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Schnell.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Schnell.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16. Note that outputs will not be deterministic when this is enabled, even if you set a seed.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Schnell.Megapixels = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Schnell.Megapixels("1"),
        description="Approximate number of megapixels for generated image",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Flux_Schnell.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Schnell.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio for the generated image",
        )
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Schnell.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Schnell.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=4,
        description="Number of denoising steps. 4 is recommended, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Schnell

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Flux_Schnell_Lora(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    The fastest image generation model tailored for fine-tuned use
    """

    Megapixels: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Megapixels
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Run faster predictions with model optimized for speed (currently fp8 quantized); disable to run in original bf16. Note that outputs will not be deterministic when this is enabled, even if you set a seed.",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. For go_fast we apply a 1.5x multiplier to this value; we've generally seen good performance when scaling the base value by that amount. You may still need to experiment to find the best value for your particular lora.",
    )
    megapixels: nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Megapixels = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Megapixels(
                "1"
            ),
            description="Approximate number of megapixels for generated image",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs to generate"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image",
    )
    lora_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports Replicate models in the format <owner>/<username> or <owner>/<username>/<version>, HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet. For example, 'fofr/flux-pixar-cars'",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=4,
        description="Number of denoising steps. 4 is recommended, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Flux_Schnell_Lora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class GPT_Image_1_5(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    OpenAI's latest image generation model with better instruction following and adherence to prompts
    """

    Quality: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Quality
    )
    Background: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Background
    )
    Moderation: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Moderation
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Output_format
    )
    Input_fidelity: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Input_fidelity
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="A text description of the desired image"
    )
    quality: nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Quality = Field(
        default=nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Quality("auto"),
        description="The quality of the generated image",
    )
    user_id: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="An optional unique identifier representing your end-user. This helps OpenAI monitor and detect abuse.",
    )
    background: nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Background = (
        Field(
            default=nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Background(
                "auto"
            ),
            description="Set whether the background is transparent or opaque or choose automatically",
        )
    )
    moderation: nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Moderation = (
        Field(
            default=nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Moderation(
                "auto"
            ),
            description="Content moderation level",
        )
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Aspect_ratio(
                "1:1"
            ),
            description="The aspect ratio of the generated image",
        )
    )
    input_images: list | OutputHandle[list] | None = connect_field(
        default=None, description="A list of images to use as input for the generation"
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Output_format(
            "webp"
        ),
        description="Output format",
    )
    input_fidelity: (
        nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Input_fidelity
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.GPT_Image_1_5.Input_fidelity(
            "low"
        ),
        description="Control how much effort the model will exert to match the style and features, especially facial features, of input images",
    )
    openai_api_key: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Your OpenAI API key (optional - uses proxy if not provided)",
    )
    number_of_images: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate (1-10)"
    )
    output_compression: int | OutputHandle[int] = connect_field(
        default=90, description="Compression level (0-100%)"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.GPT_Image_1_5

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Hyper_Flux_8Step(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    Hyper FLUX 8-step by ByteDance
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Hyper_Flux_8Step.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Hyper_Flux_8Step.Output_format
    )

    seed: int | OutputHandle[int] = connect_field(
        default=0, description="Random seed. Set for reproducible generation"
    )
    width: int | OutputHandle[int] = connect_field(
        default=848,
        description="Width of the generated image. Optional, only used when aspect_ratio=custom. Must be a multiple of 16 (if it's not, it will be rounded to nearest multiple of 16)",
    )
    height: int | OutputHandle[int] = connect_field(
        default=848,
        description="Height of the generated image. Optional, only used when aspect_ratio=custom. Must be a multiple of 16 (if it's not, it will be rounded to nearest multiple of 16)",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Hyper_Flux_8Step.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Hyper_Flux_8Step.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio for the generated image. The size will always be 1 megapixel, i.e. 1024x1024 if aspect ratio is 1:1. To use arbitrary width and height, set aspect ratio to 'custom'.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Hyper_Flux_8Step.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Hyper_Flux_8Step.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="Guidance scale for the diffusion process"
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See [https://replicate.com/docs/how-does-replicate-work#safety](https://replicate.com/docs/how-does-replicate-work#safety)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Hyper_Flux_8Step

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Ideogram_V2(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    An excellent image model with state of the art inpainting, prompt comprehension and text rendering
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2.Resolution
    )
    Style_type: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2.Style_type
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2.Aspect_ratio
    )
    Magic_prompt_option: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2.Magic_prompt_option
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="A black and white image. Black pixels are inpainted, white pixels are preserved. The mask will be resized to match the image size.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image file to use for inpainting. You must also use a mask.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    resolution: nodetool.nodes.replicate.image.generate.Ideogram_V2.Resolution = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2.Resolution("None"),
        description="Resolution. Overrides aspect ratio. Ignored if an inpainting image is given.",
    )
    style_type: nodetool.nodes.replicate.image.generate.Ideogram_V2.Style_type = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2.Style_type("None"),
        description="The styles help define the specific aesthetic of the image you want to generate.",
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Ideogram_V2.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Ideogram_V2.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio. Ignored if a resolution or inpainting image is given.",
        )
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Things you do not want to see in the generated image.",
    )
    magic_prompt_option: (
        nodetool.nodes.replicate.image.generate.Ideogram_V2.Magic_prompt_option
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2.Magic_prompt_option(
            "Auto"
        ),
        description="Magic Prompt will interpret your prompt and optimize it to maximize variety and quality of the images generated. You can also use it to write prompts in different languages.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Ideogram_V2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Ideogram_V2A(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Like Ideogram v2, but faster and cheaper
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2A.Resolution
    )
    Style_type: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2A.Style_type
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2A.Aspect_ratio
    )
    Magic_prompt_option: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2A.Magic_prompt_option
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    resolution: nodetool.nodes.replicate.image.generate.Ideogram_V2A.Resolution = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2A.Resolution("None"),
        description="Resolution. Overrides aspect ratio. Ignored if an inpainting image is given.",
    )
    style_type: nodetool.nodes.replicate.image.generate.Ideogram_V2A.Style_type = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2A.Style_type("None"),
        description="The styles help define the specific aesthetic of the image you want to generate.",
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Ideogram_V2A.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Ideogram_V2A.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio. Ignored if a resolution or inpainting image is given.",
        )
    )
    magic_prompt_option: (
        nodetool.nodes.replicate.image.generate.Ideogram_V2A.Magic_prompt_option
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2A.Magic_prompt_option(
            "Auto"
        ),
        description="Magic Prompt will interpret your prompt and optimize it to maximize variety and quality of the images generated. You can also use it to write prompts in different languages.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Ideogram_V2A

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Ideogram_V2_Turbo(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    A fast image model with state of the art inpainting, prompt comprehension and text rendering.
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Resolution
    )
    Style_type: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Style_type
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Aspect_ratio
    )
    Magic_prompt_option: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Magic_prompt_option
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="A black and white image. Black pixels are inpainted, white pixels are preserved. The mask will be resized to match the image size.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image file to use for inpainting. You must also use a mask.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    resolution: nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Resolution = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Resolution(
                "None"
            ),
            description="Resolution. Overrides aspect ratio. Ignored if an inpainting image is given.",
        )
    )
    style_type: nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Style_type = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Style_type(
                "None"
            ),
            description="The styles help define the specific aesthetic of the image you want to generate.",
        )
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio. Ignored if a resolution or inpainting image is given.",
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Things you do not want to see in the generated image.",
    )
    magic_prompt_option: (
        nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Magic_prompt_option
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo.Magic_prompt_option(
            "Auto"
        ),
        description="Magic Prompt will interpret your prompt and optimize it to maximize variety and quality of the images generated. You can also use it to write prompts in different languages.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Ideogram_V2_Turbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Ideogram_V3_Turbo(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    Turbo is the fastest and cheapest Ideogram v3. v3 creates images with stunning realism, creative designs, and consistent styles
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Resolution
    )
    Style_type: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Style_type
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Aspect_ratio
    )
    Style_preset: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Style_preset
    )
    Magic_prompt_option: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Magic_prompt_option
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="A black and white image. Black pixels are inpainted, white pixels are preserved. The mask will be resized to match the image size.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image file to use for inpainting. You must also use a mask.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    resolution: nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Resolution = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Resolution(
                "None"
            ),
            description="Resolution. Overrides aspect ratio. Ignored if an inpainting image is given.",
        )
    )
    style_type: nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Style_type = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Style_type(
                "None"
            ),
            description="The styles help define the specific aesthetic of the image you want to generate.",
        )
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Aspect_ratio(
            "1:1"
        ),
        description="Aspect ratio. Ignored if a resolution or inpainting image is given.",
    )
    style_preset: (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Style_preset
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Style_preset(
            "None"
        ),
        description="Apply a predefined artistic style to the generated image (V3 models only).",
    )
    magic_prompt_option: (
        nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Magic_prompt_option
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo.Magic_prompt_option(
            "Auto"
        ),
        description="Magic Prompt will interpret your prompt and optimize it to maximize variety and quality of the images generated. You can also use it to write prompts in different languages.",
    )
    style_reference_images: list | OutputHandle[list] | None = connect_field(
        default=None, description="A list of images to use as style references."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Ideogram_V3_Turbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Illusions(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Create illusions with img2img and masking support
    """

    Sizing_strategy: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Illusions.Sizing_strategy
    )

    seed: int | OutputHandle[int] | None = connect_field(default=None, description=None)
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional img2img",
    )
    width: int | OutputHandle[int] = connect_field(default=768, description=None)
    height: int | OutputHandle[int] = connect_field(default=768, description=None)
    prompt: str | OutputHandle[str] = connect_field(
        default="a painting of a 19th century town", description=None
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional mask for inpainting",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of outputs"
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Control image",
    )
    controlnet_end: float | OutputHandle[float] = connect_field(
        default=1.0, description="When controlnet conditioning ends"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="ugly, disfigured, low quality, blurry, nsfw",
        description="The negative prompt to guide image generation.",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img / inpaint. 1.0 corresponds to full destruction of information in image",
    )
    sizing_strategy: (
        nodetool.nodes.replicate.image.generate.Illusions.Sizing_strategy
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Illusions.Sizing_strategy(
            "width/height"
        ),
        description="Decide how to resize images  use width/height, resize based on input image or control image",
    )
    controlnet_start: float | OutputHandle[float] = connect_field(
        default=0.0, description="When controlnet conditioning starts"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="Number of diffusion steps"
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=0.75, description="How strong the controlnet conditioning is"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Illusions

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Imagen_3(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Google's highest quality text-to-image model, capable of generating images with detail, rich lighting and beauty
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Imagen_3.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Imagen_3.Output_format
    )
    Safety_filter_level: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Imagen_3.Safety_filter_level
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Imagen_3.Aspect_ratio = Field(
        default=nodetool.nodes.replicate.image.generate.Imagen_3.Aspect_ratio("1:1"),
        description="Aspect ratio of the generated image",
    )
    output_format: nodetool.nodes.replicate.image.generate.Imagen_3.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Imagen_3.Output_format(
                "jpg"
            ),
            description="Format of the output image",
        )
    )
    safety_filter_level: (
        nodetool.nodes.replicate.image.generate.Imagen_3.Safety_filter_level
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Imagen_3.Safety_filter_level(
            "block_only_high"
        ),
        description="block_low_and_above is strictest, block_medium_and_above blocks some prompts, block_only_high is most permissive but some prompts will still be blocked",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Imagen_3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Imagen_4_Fast(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Use this fast version of Imagen 4 when speed and cost are more important than quality
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Output_format
    )
    Safety_filter_level: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Safety_filter_level
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Aspect_ratio(
                "1:1"
            ),
            description="Aspect ratio of the generated image",
        )
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Output_format(
            "jpg"
        ),
        description="Format of the output image",
    )
    safety_filter_level: (
        nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Safety_filter_level
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Imagen_4_Fast.Safety_filter_level(
            "block_only_high"
        ),
        description="block_low_and_above is strictest, block_medium_and_above blocks some prompts, block_only_high is most permissive but some prompts will still be blocked",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Imagen_4_Fast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Kandinsky(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    multilingual text2image latent diffusion model
    """

    Width: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Kandinsky.Width
    )
    Height: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Kandinsky.Height
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Kandinsky.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    width: nodetool.nodes.replicate.image.generate.Kandinsky.Width = Field(
        default=nodetool.nodes.replicate.image.generate.Kandinsky.Width(512),
        description="Width of output image. Lower the setting if hits memory limits.",
    )
    height: nodetool.nodes.replicate.image.generate.Kandinsky.Height = Field(
        default=nodetool.nodes.replicate.image.generate.Kandinsky.Height(512),
        description="Height of output image. Lower the setting if hits memory limits.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A moss covered astronaut with a black background",
        description="Input prompt",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    output_format: nodetool.nodes.replicate.image.generate.Kandinsky.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Kandinsky.Output_format(
                "webp"
            ),
            description="Output image format",
        )
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Specify things to not see in the output"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=75, description="Number of denoising steps"
    )
    num_inference_steps_prior: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps for priors"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Kandinsky

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Kandinsky_2_2(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    multilingual text2image latent diffusion model
    """

    Width: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Width
    )
    Height: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Height
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    width: nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Width = Field(
        default=nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Width(512),
        description="Width of output image. Lower the setting if hits memory limits.",
    )
    height: nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Height = Field(
        default=nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Height(512),
        description="Height of output image. Lower the setting if hits memory limits.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A moss covered astronaut with a black background",
        description="Input prompt",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Kandinsky_2_2.Output_format(
            "webp"
        ),
        description="Output image format",
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Specify things to not see in the output"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=75, description="Number of denoising steps"
    )
    num_inference_steps_prior: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps for priors"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Kandinsky_2_2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Minimax_Image_01(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    Minimax's first image model, with character reference support
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Minimax_Image_01.Aspect_ratio
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for generation"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Minimax_Image_01.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Minimax_Image_01.Aspect_ratio(
            "1:1"
        ),
        description="Image aspect ratio",
    )
    number_of_images: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate"
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use prompt optimizer"
    )
    subject_reference: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="An optional character reference image (human face) to use as the subject in the generated image(s).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Minimax_Image_01

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Photon_Flash(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Accelerated variant of Photon prioritizing speed while maintaining quality
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Photon_Flash.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Photon_Flash.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Photon_Flash.Aspect_ratio(
                "16:9"
            ),
            description="Aspect ratio of the generated image",
        )
    )
    image_reference: str | OutputHandle[str] | None = connect_field(
        default=None, description="Reference image to guide generation"
    )
    style_reference: str | OutputHandle[str] | None = connect_field(
        default=None, description="Style reference image to guide generation"
    )
    character_reference: str | OutputHandle[str] | None = connect_field(
        default=None, description="Character reference image to guide generation"
    )
    image_reference_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Deprecated: Use image_reference instead",
    )
    style_reference_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Deprecated: Use style_reference instead",
    )
    image_reference_weight: float | OutputHandle[float] = connect_field(
        default=0.85,
        description="Weight of the reference image. Larger values will make the reference image have a stronger influence on the generated image.",
    )
    style_reference_weight: float | OutputHandle[float] = connect_field(
        default=0.85, description="Weight of the style reference image"
    )
    character_reference_url: types.ImageRef | OutputHandle[types.ImageRef] = (
        connect_field(
            default=types.ImageRef(
                type="image", uri="", asset_id=None, data=None, metadata=None
            ),
            description="Deprecated: Use character_reference instead",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Photon_Flash

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class PlaygroundV2(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Playground v2.5 is the state-of-the-art open-source model in aesthetic quality
    """

    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.PlaygroundV2.Scheduler
    )

    mask: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img or inpaint mode",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of output image"
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of output image"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",
        description="Input prompt",
    )
    scheduler: nodetool.nodes.replicate.image.generate.PlaygroundV2.Scheduler = Field(
        default=nodetool.nodes.replicate.image.generate.PlaygroundV2.Scheduler(
            "DPMSolver++"
        ),
        description="Scheduler. DPMSolver++ or DPM++2MKarras is recommended for most cases",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="Scale for classifier-free guidance"
    )
    apply_watermark: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Applies a watermark to enable determining if an image is generated in downstream applications. If you have other provisions for generating or deploying images safely, you can use this to disable watermarking.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="ugly, deformed, noisy, blurry, distorted",
        description="Negative Input prompt",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img / inpaint. 1.0 corresponds to full destruction of information in image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See https://replicate.com/docs/how-does-replicate-work#safety",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.PlaygroundV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Proteus_V_02(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Proteus v0.2 shows subtle yet significant improvements over Version 0.1. It demonstrates enhanced prompt understanding that surpasses MJ6, while also approaching its stylistic capabilities.
    """

    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Proteus_V_02.Scheduler
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img or inpaint mode",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of output image"
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of output image"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="black fluffy gorgeous dangerous cat animal creature, large orange eyes, big fluffy ears, piercing gaze, full moon, dark ambiance, best quality, extremely detailed",
        description="Input prompt",
    )
    scheduler: nodetool.nodes.replicate.image.generate.Proteus_V_02.Scheduler = Field(
        default=nodetool.nodes.replicate.image.generate.Proteus_V_02.Scheduler(
            "KarrasDPM"
        ),
        description="scheduler",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance. Recommended 7-8"
    )
    apply_watermark: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Applies a watermark to enable determining if an image is generated in downstream applications. If you have other provisions for generating or deploying images safely, you can use this to disable watermarking.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, low quality", description="Negative Input prompt"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img / inpaint. 1.0 corresponds to full destruction of information in image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=20,
        description="Number of denoising steps. 20 to 35 steps for more detail, 20 steps for faster results.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See https://replicate.com/docs/how-does-replicate-work#safety",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Proteus_V_02

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Proteus_V_03(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    ProteusV0.3: The Anime Update
    """

    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Proteus_V_03.Scheduler
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img or inpaint mode",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of output image. Recommended 1024 or 1280"
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of output image. Recommended 1024 or 1280"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Anime full body portrait of a swordsman holding his weapon in front of him. He is facing the camera with a fierce look on his face. Anime key visual (best quality, HD, ~+~aesthetic~+~:1.2)",
        description="Input prompt",
    )
    scheduler: nodetool.nodes.replicate.image.generate.Proteus_V_03.Scheduler = Field(
        default=nodetool.nodes.replicate.image.generate.Proteus_V_03.Scheduler(
            "DPM++2MSDE"
        ),
        description="scheduler",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance. Recommended 7-8"
    )
    apply_watermark: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Applies a watermark to enable determining if an image is generated in downstream applications. If you have other provisions for generating or deploying images safely, you can use this to disable watermarking.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, low quality", description="Negative Input prompt"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img / inpaint. 1.0 corresponds to full destruction of information in image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=20,
        description="Number of denoising steps. 20 to 60 steps for more detail, 20 steps for faster results.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See https://replicate.com/docs/how-does-replicate-work#safety",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Proteus_V_03

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class PulidBase(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Use a face to make images. Uses SDXL fine-tuned checkpoints.
    """

    Face_style: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.PulidBase.Face_style
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.PulidBase.Output_format
    )
    Checkpoint_model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.PulidBase.Checkpoint_model
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Set a seed for reproducibility. Random by default."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Width of the output image (ignored if structure image given)",
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Height of the output image (ignored if structure image given)",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A photo of a person",
        description="You might need to include a gender in the prompt to get the desired result",
    )
    face_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The face image to use for the generation",
    )
    face_style: nodetool.nodes.replicate.image.generate.PulidBase.Face_style = Field(
        default=nodetool.nodes.replicate.image.generate.PulidBase.Face_style(
            "high-fidelity"
        ),
        description="Style of the face",
    )
    output_format: nodetool.nodes.replicate.image.generate.PulidBase.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.PulidBase.Output_format(
                "webp"
            ),
            description="Format of the output images",
        )
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality of the output images, from 0 to 100. 100 is best quality, 0 is lowest quality.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Things you do not want to see in your image"
    )
    checkpoint_model: (
        nodetool.nodes.replicate.image.generate.PulidBase.Checkpoint_model
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.PulidBase.Checkpoint_model(
            "general - dreamshaperXL_alpha2Xl10"
        ),
        description="Model to use for the generation",
    )
    number_of_images: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.PulidBase

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Qwen_Image(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    An image generation foundation model in the Qwen series that achieves significant advances in complex text rendering.
    """

    Image_size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Qwen_Image.Image_size
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Qwen_Image.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Qwen_Image.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img pipeline",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for generated image"
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Run faster predictions with additional optimizations.",
    )
    guidance: float | OutputHandle[float] = connect_field(
        default=3,
        description="Guidance for generated image. Lower values can give more realistic images. Good values to try are 2, 2.5, 3 and 3.5",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.9, description="Strength for img2img pipeline"
    )
    image_size: nodetool.nodes.replicate.image.generate.Qwen_Image.Image_size = Field(
        default=nodetool.nodes.replicate.image.generate.Qwen_Image.Image_size(
            "optimize_for_quality"
        ),
        description="Image size for the generated image",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied.",
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Qwen_Image.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Qwen_Image.Aspect_ratio(
                "16:9"
            ),
            description="Aspect ratio for the generated image",
        )
    )
    lora_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Only works with text to image pipeline. Supports arbitrary .safetensors URLs, tar files, and zip files from the Internet (for example, 'https://huggingface.co/Viktor1717/scandinavian-interior-style1/resolve/main/my_first_flux_lora_v1.safetensors', 'https://example.com/lora_weights.tar.gz', or 'https://example.com/lora_weights.zip')",
    )
    output_format: nodetool.nodes.replicate.image.generate.Qwen_Image.Output_format = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Qwen_Image.Output_format(
                "webp"
            ),
            description="Format of the output images",
        )
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enhance the prompt with positive magic."
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default=" ", description="Negative prompt for generated image"
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights from Replicate training. Only works with text to image pipeline. Supports arbitrary .safetensors URLs, tar files, and zip files from the Internet.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of denoising steps. Recommended range is 28-50, and lower number of steps produce lower quality outputs, faster.",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Qwen_Image

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Qwen_Image_Edit(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Edit images using a prompt. This model extends Qwen-Images unique text rendering capabilities to image editing tasks, enabling precise text editing
    """

    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Qwen_Image_Edit.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as reference. Must be jpeg, png, gif, or webp.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text instruction on how to edit the given image."
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Run faster predictions with additional optimizations.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.Qwen_Image_Edit.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Qwen_Image_Edit.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality when saving the output images, from 0 to 100. 100 is best quality, 0 is lowest quality. Not relevant for .png outputs",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Qwen_Image_Edit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Recraft_20B(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Affordable and fast images
    """

    Size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_20B.Size
    )
    Style: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_20B.Style
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_20B.Aspect_ratio
    )

    size: nodetool.nodes.replicate.image.generate.Recraft_20B.Size = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_20B.Size("1024x1024"),
        description="Width and height of the generated image. Size is ignored if an aspect ratio is set.",
    )
    style: nodetool.nodes.replicate.image.generate.Recraft_20B.Style = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_20B.Style(
            "realistic_image"
        ),
        description="Style of the generated image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Recraft_20B.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Recraft_20B.Aspect_ratio(
                "Not set"
            ),
            description="Aspect ratio of the generated image",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Recraft_20B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Recraft_20B_SVG(SingleOutputGraphNode[types.SVGRef], GraphNode[types.SVGRef]):
    """
    Affordable and fast vector images
    """

    Size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Size
    )
    Style: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Style
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Aspect_ratio
    )

    size: nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Size = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Size(
            "1024x1024"
        ),
        description="Width and height of the generated image. Size is ignored if an aspect ratio is set.",
    )
    style: nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Style = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Style(
            "vector_illustration"
        ),
        description="Style of the generated image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_20B_SVG.Aspect_ratio(
            "Not set"
        ),
        description="Aspect ratio of the generated image",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Recraft_20B_SVG

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Recraft_V3(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Recraft V3 (code-named red_panda) is a text-to-image model with the ability to generate long texts, and images in a wide list of styles. As of today, it is SOTA in image generation, proven by the Text-to-Image Benchmark by Artificial Analysis
    """

    Size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_V3.Size
    )
    Style: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_V3.Style
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_V3.Aspect_ratio
    )

    size: nodetool.nodes.replicate.image.generate.Recraft_V3.Size = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_V3.Size("1024x1024"),
        description="Width and height of the generated image. Size is ignored if an aspect ratio is set.",
    )
    style: nodetool.nodes.replicate.image.generate.Recraft_V3.Style = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_V3.Style("any"),
        description="Style of the generated image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Recraft_V3.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Recraft_V3.Aspect_ratio(
                "Not set"
            ),
            description="Aspect ratio of the generated image",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Recraft_V3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Recraft_V3_SVG(SingleOutputGraphNode[types.SVGRef], GraphNode[types.SVGRef]):
    """
    Recraft V3 SVG (code-named red_panda) is a text-to-image model with the ability to generate high quality SVG images including logotypes, and icons. The model supports a wide list of styles.
    """

    Size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Size
    )
    Style: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Style
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Aspect_ratio
    )

    size: nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Size = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Size(
            "1024x1024"
        ),
        description="Width and height of the generated image. Size is ignored if an aspect ratio is set.",
    )
    style: nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Style = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Style("any"),
        description="Style of the generated image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Recraft_V3_SVG.Aspect_ratio(
            "Not set"
        ),
        description="Aspect ratio of the generated image",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Recraft_V3_SVG

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class SDXL_Ad_Inpaint(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Product advertising image generator using SDXL
    """

    Img_size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Img_size
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Scheduler
    )
    Product_fill: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Product_fill
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Empty or 0 for a random image"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Remove background from this image",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Describe the new setting for your product"
    )
    img_size: nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Img_size = Field(
        default=nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Img_size(
            "1024, 1024"
        ),
        description="Possible SDXL image sizes",
    )
    apply_img: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Applies the original product image to the final result",
    )
    scheduler: nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Scheduler = (
        Field(
            default=nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Scheduler(
                "K_EULER"
            ),
            description="scheduler",
        )
    )
    product_fill: (
        nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Product_fill
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint.Product_fill(
            "Original"
        ),
        description="What percentage of the image width to fill with product",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance Scale"
    )
    condition_scale: float | OutputHandle[float] = connect_field(
        default=0.9, description="controlnet conditioning scale for generalization"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, out of frame, illustration, 3d, sepia, painting, cartoons, sketch, watermark, text, Logo, advertisement",
        description="Describe what you do not want in your setting",
    )
    num_refine_steps: int | OutputHandle[int] = connect_field(
        default=10, description="Number of steps to refine"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="Inference Steps"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.SDXL_Ad_Inpaint

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class SDXL_Controlnet(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    SDXL ControlNet - Canny
    """

    seed: int | OutputHandle[int] = connect_field(
        default=0, description="Random seed. Set to 0 to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img or inpaint mode",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="aerial view, a futuristic research complex in a bright foggy jungle, hard lighting",
        description="Input prompt",
    )
    condition_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="controlnet conditioning scale for generalization"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, bad quality, sketches",
        description="Input Negative Prompt",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.SDXL_Controlnet

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class SDXL_Emoji(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    An SDXL fine-tune based on Apple Emojis
    """

    Refine: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.SDXL_Emoji.Refine
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.SDXL_Emoji.Scheduler
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img or inpaint mode",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of output image"
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of output image"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="An astronaut riding a rainbow unicorn", description="Input prompt"
    )
    refine: nodetool.nodes.replicate.image.generate.SDXL_Emoji.Refine = Field(
        default=nodetool.nodes.replicate.image.generate.SDXL_Emoji.Refine("no_refiner"),
        description="Which refine style to use",
    )
    scheduler: nodetool.nodes.replicate.image.generate.SDXL_Emoji.Scheduler = Field(
        default=nodetool.nodes.replicate.image.generate.SDXL_Emoji.Scheduler("K_EULER"),
        description="scheduler",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="LoRA additive scale. Only applicable on trained models.",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    refine_steps: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="For base_image_refiner, the number of steps to refine, defaults to num_inference_steps",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance"
    )
    apply_watermark: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Applies a watermark to enable determining if an image is generated in downstream applications. If you have other provisions for generating or deploying images safely, you can use this to disable watermarking.",
    )
    high_noise_frac: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="For expert_ensemble_refiner, the fraction of noise to use",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Input Negative Prompt"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img / inpaint. 1.0 corresponds to full destruction of information in image",
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Replicate LoRA weights to use. Leave blank to use the default weights.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See https://replicate.com/docs/how-does-replicate-work#safety",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.SDXL_Emoji

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class SDXL_Pixar(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Create Pixar poster easily with SDXL Pixar.
    """

    Refine: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.SDXL_Pixar.Refine
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.SDXL_Pixar.Scheduler
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img or inpaint mode",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of output image"
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of output image"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="An astronaut riding a rainbow unicorn", description="Input prompt"
    )
    refine: nodetool.nodes.replicate.image.generate.SDXL_Pixar.Refine = Field(
        default=nodetool.nodes.replicate.image.generate.SDXL_Pixar.Refine("no_refiner"),
        description="Which refine style to use",
    )
    scheduler: nodetool.nodes.replicate.image.generate.SDXL_Pixar.Scheduler = Field(
        default=nodetool.nodes.replicate.image.generate.SDXL_Pixar.Scheduler("K_EULER"),
        description="scheduler",
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="LoRA additive scale. Only applicable on trained models.",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    refine_steps: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="For base_image_refiner, the number of steps to refine, defaults to num_inference_steps",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance"
    )
    apply_watermark: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Applies a watermark to enable determining if an image is generated in downstream applications. If you have other provisions for generating or deploying images safely, you can use this to disable watermarking.",
    )
    high_noise_frac: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="For expert_ensemble_refiner, the fraction of noise to use",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Input Negative Prompt"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img / inpaint. 1.0 corresponds to full destruction of information in image",
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Replicate LoRA weights to use. Leave blank to use the default weights.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See https://replicate.com/docs/how-does-replicate-work#safety",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.SDXL_Pixar

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class Seedream_4(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Unified text-to-image generation and precise single-sentence editing at up to 4K resolution
    """

    Size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Seedream_4.Size
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Seedream_4.Aspect_ratio
    )
    Sequential_image_generation: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.Seedream_4.Sequential_image_generation
    )

    size: nodetool.nodes.replicate.image.generate.Seedream_4.Size = Field(
        default=nodetool.nodes.replicate.image.generate.Seedream_4.Size("2K"),
        description="Image resolution: 1K (1024px), 2K (2048px), 4K (4096px), or 'custom' for specific dimensions.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=2048,
        description="Custom image width (only used when size='custom'). Range: 1024-4096 pixels.",
    )
    height: int | OutputHandle[int] = connect_field(
        default=2048,
        description="Custom image height (only used when size='custom'). Range: 1024-4096 pixels.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    max_images: int | OutputHandle[int] = connect_field(
        default=1,
        description="Maximum number of images to generate when sequential_image_generation='auto'. Range: 1-15. Total images (input + generated) cannot exceed 15.",
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[],
        description="Input image(s) for image-to-image generation. List of 1-10 images for single or multi-reference generation.",
    )
    aspect_ratio: nodetool.nodes.replicate.image.generate.Seedream_4.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.image.generate.Seedream_4.Aspect_ratio(
                "match_input_image"
            ),
            description="Image aspect ratio. Only used when size is not 'custom'. Use 'match_input_image' to automatically match the input image's aspect ratio.",
        )
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable prompt enhancement for higher quality results, this will take longer to generate.",
    )
    sequential_image_generation: (
        nodetool.nodes.replicate.image.generate.Seedream_4.Sequential_image_generation
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.Seedream_4.Sequential_image_generation(
            "disabled"
        ),
        description="Group image generation mode. 'disabled' generates a single image. 'auto' lets the model decide whether to generate multiple related images (e.g., story scenes, character variations).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.Seedream_4

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StableDiffusion(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    A latent text-to-image diffusion model capable of generating photo-realistic images given any text input
    """

    Width: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion.Width
    )
    Height: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion.Height
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion.Scheduler
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    width: nodetool.nodes.replicate.image.generate.StableDiffusion.Width = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion.Width(768),
        description="Width of generated image in pixels. Needs to be a multiple of 64",
    )
    height: nodetool.nodes.replicate.image.generate.StableDiffusion.Height = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion.Height(768),
        description="Height of generated image in pixels. Needs to be a multiple of 64",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="a vision of paradise. unreal engine", description="Input prompt"
    )
    scheduler: nodetool.nodes.replicate.image.generate.StableDiffusion.Scheduler = (
        Field(
            default=nodetool.nodes.replicate.image.generate.StableDiffusion.Scheduler(
                "DPMSolverMultistep"
            ),
            description="Choose a scheduler.",
        )
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance"
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Specify things to not see in the output"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StableDiffusion

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StableDiffusion3_5_Large(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    A text-to-image model that generates high-resolution images with fine details. It supports various artistic styles and produces diverse outputs from the same prompt, thanks to Query-Key Normalization.
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large.Output_format
    )

    cfg: float | OutputHandle[float] = connect_field(
        default=5,
        description="The guidance scale tells the model how similar the output should be to the prompt.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Set a seed for reproducibility. Random by default."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image mode. The aspect ratio of your output will match this image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large.Aspect_ratio(
            "1:1"
        ),
        description="The aspect ratio of your output image. This value is ignored if you are using an input image.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="What you do not want to see in the image"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.85,
        description="Prompt strength (or denoising strength) when using image to image. 1.0 corresponds to full destruction of information in image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StableDiffusion3_5_Large_Turbo(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    A text-to-image model that generates high-resolution images with fine details. It supports various artistic styles and produces diverse outputs from the same prompt, with a focus on fewer inference steps
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large_Turbo.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large_Turbo.Output_format
    )

    cfg: float | OutputHandle[float] = connect_field(
        default=1,
        description="The guidance scale tells the model how similar the output should be to the prompt.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Set a seed for reproducibility. Random by default."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image mode. The aspect ratio of your output will match this image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large_Turbo.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large_Turbo.Aspect_ratio(
            "1:1"
        ),
        description="The aspect ratio of your output image. This value is ignored if you are using an input image.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large_Turbo.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large_Turbo.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="What you do not want to see in the image"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.85,
        description="Prompt strength (or denoising strength) when using image to image. 1.0 corresponds to full destruction of information in image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Large_Turbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StableDiffusion3_5_Medium(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    2.5 billion parameter image model with improved MMDiT-X architecture
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Medium.Aspect_ratio
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Medium.Output_format
    )

    cfg: float | OutputHandle[float] = connect_field(
        default=5,
        description="The guidance scale tells the model how similar the output should be to the prompt.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Set a seed for reproducibility. Random by default."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for image to image mode. The aspect ratio of your output will match this image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    aspect_ratio: (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Medium.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Medium.Aspect_ratio(
            "1:1"
        ),
        description="The aspect ratio of your output image. This value is ignored if you are using an input image.",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Medium.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Medium.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="What you do not want to see in the image"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.85,
        description="Prompt strength (or denoising strength) when using image to image. 1.0 corresponds to full destruction of information in image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StableDiffusion3_5_Medium

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StableDiffusionInpainting(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    Fill in masked parts of images with Stable Diffusion
    """

    Width: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Width
    )
    Height: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Height
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Scheduler
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Black and white image to use as mask for inpainting over the image provided. White pixels are inpainted and black pixels are preserved.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Initial image to generate variations of. Will be resized to height x width",
    )
    width: nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Width = (
        Field(
            default=nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Width(
                512
            ),
            description="Width of generated image in pixels. Needs to be a multiple of 64",
        )
    )
    height: nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Height = (
        Field(
            default=nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Height(
                512
            ),
            description="Height of generated image in pixels. Needs to be a multiple of 64",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="a vision of paradise. unreal engine", description="Input prompt"
    )
    scheduler: (
        nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Scheduler
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusionInpainting.Scheduler(
            "DPMSolverMultistep"
        ),
        description="Choose a scheduler.",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance"
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Specify things to not see in the output"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See [https://replicate.com/docs/how-does-replicate-work#safety](https://replicate.com/docs/how-does-replicate-work#safety)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StableDiffusionInpainting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StableDiffusionXL(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    A text-to-image generative AI model that creates beautiful images
    """

    Refine: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusionXL.Refine
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusionXL.Scheduler
    )

    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted.",
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image for img2img or inpaint mode",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of output image"
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of output image"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="An astronaut riding a rainbow unicorn", description="Input prompt"
    )
    refine: nodetool.nodes.replicate.image.generate.StableDiffusionXL.Refine = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusionXL.Refine(
            "no_refiner"
        ),
        description="Which refine style to use",
    )
    scheduler: nodetool.nodes.replicate.image.generate.StableDiffusionXL.Scheduler = (
        Field(
            default=nodetool.nodes.replicate.image.generate.StableDiffusionXL.Scheduler(
                "K_EULER"
            ),
            description="scheduler",
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="LoRA additive scale. Only applicable on trained models.",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    refine_steps: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="For base_image_refiner, the number of steps to refine, defaults to num_inference_steps",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Scale for classifier-free guidance"
    )
    apply_watermark: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Applies a watermark to enable determining if an image is generated in downstream applications. If you have other provisions for generating or deploying images safely, you can use this to disable watermarking.",
    )
    high_noise_frac: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="For expert_ensemble_refiner, the fraction of noise to use",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Input Negative Prompt"
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Prompt strength when using img2img / inpaint. 1.0 corresponds to full destruction of information in image",
    )
    replicate_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Replicate LoRA weights to use. Leave blank to use the default weights.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Disable safety checker for generated images. This feature is only available through the API. See [https://replicate.com/docs/how-does-replicate-work#safety](https://replicate.com/docs/how-does-replicate-work#safety)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StableDiffusionXL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StableDiffusionXLLightning(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    SDXL-Lightning by ByteDance: a fast text-to-image model that makes high-quality images in 4 steps
    """

    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StableDiffusionXLLightning.Scheduler
    )

    seed: int | OutputHandle[int] = connect_field(
        default=0, description="Random seed. Leave blank to randomize the seed"
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of output image. Recommended 1024 or 1280"
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of output image. Recommended 1024 or 1280"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="self-portrait of a woman, lightning in the background",
        description="Input prompt",
    )
    scheduler: (
        nodetool.nodes.replicate.image.generate.StableDiffusionXLLightning.Scheduler
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StableDiffusionXLLightning.Scheduler(
            "K_EULER"
        ),
        description="scheduler",
    )
    num_outputs: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to output."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=0, description="Scale for classifier-free guidance"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, low quality", description="Negative Input prompt"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=4, description="Number of denoising steps. 4 for best results"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated images"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StableDiffusionXLLightning

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StickerMaker(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Make stickers with AI. Generates graphics with transparent backgrounds.
    """

    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StickerMaker.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Fix the random seed for reproducibility"
    )
    steps: int | OutputHandle[int] = connect_field(default=17, description=None)
    width: int | OutputHandle[int] = connect_field(default=1152, description=None)
    height: int | OutputHandle[int] = connect_field(default=1152, description=None)
    prompt: str | OutputHandle[str] = connect_field(
        default="a cute cat", description=None
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.StickerMaker.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StickerMaker.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=90,
        description="Quality of the output images, from 0 to 100. 100 is best quality, 0 is lowest quality.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Things you do not want in the image"
    )
    number_of_images: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StickerMaker

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.generate
from nodetool.workflows.base_node import BaseNode


class StyleTransfer(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Transfer the style of one image to another
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StyleTransfer.Model
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.generate.StyleTransfer.Output_format
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Set a seed for reproducibility. Random by default."
    )
    model: nodetool.nodes.replicate.image.generate.StyleTransfer.Model = Field(
        default=nodetool.nodes.replicate.image.generate.StyleTransfer.Model("fast"),
        description="Model to use for the generation",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Width of the output image (ignored if structure image given)",
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Height of the output image (ignored if structure image given)",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="An astronaut riding a unicorn", description="Prompt for the image"
    )
    style_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Copy the style from this image",
    )
    output_format: (
        nodetool.nodes.replicate.image.generate.StyleTransfer.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.image.generate.StyleTransfer.Output_format(
            "webp"
        ),
        description="Format of the output images",
    )
    output_quality: int | OutputHandle[int] = connect_field(
        default=80,
        description="Quality of the output images, from 0 to 100. 100 is best quality, 0 is lowest quality.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Things you do not want to see in your image"
    )
    structure_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An optional image to copy structure from. Output images will use the same aspect ratio.",
    )
    number_of_images: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate"
    )
    structure_depth_strength: float | OutputHandle[float] = connect_field(
        default=1, description="Strength of the depth controlnet"
    )
    structure_denoising_strength: float | OutputHandle[float] = connect_field(
        default=0.65,
        description="How much of the original image (and colors) to preserve (0 is all, 1 is none, 0.65 is a good balance)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.generate.StyleTransfer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
