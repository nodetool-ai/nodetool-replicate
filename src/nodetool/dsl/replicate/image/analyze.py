# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class Blip(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Generate image captions
    """

    Task: typing.ClassVar[type] = nodetool.nodes.replicate.image.analyze.Blip.Task

    task: nodetool.nodes.replicate.image.analyze.Blip.Task = Field(
        default=nodetool.nodes.replicate.image.analyze.Blip.Task("image_captioning"),
        description="Choose a task.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image",
    )
    caption: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Type caption for the input image for image text matching task.",
    )
    question: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Type question for the input image for visual question answering task.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.Blip

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class Blip2(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Answers questions about images
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to query or caption",
    )
    caption: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Select if you want to generate image captions instead of asking questions",
    )
    context: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Optional - previous questions and answers to be used as context for answering current question",
    )
    question: str | OutputHandle[str] = connect_field(
        default="What is this a picture of?",
        description="Question to ask about this image. Leave blank for captioning",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=1, description="Temperature for use with nucleus sampling"
    )
    use_nucleus_sampling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Toggles the model using nucleus sampling to generate responses",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.Blip2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class ClipFeatures(SingleOutputGraphNode[list[dict]], GraphNode[list[dict]]):
    """
    Return CLIP features for the clip-vit-large-patch14 model
    """

    inputs: str | OutputHandle[str] = connect_field(
        default="a\nb",
        description="Newline-separated inputs. Can either be strings of text or image URIs starting with http[s]://",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.ClipFeatures

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class ClipInterrogator(SingleOutputGraphNode[str], GraphNode[str]):
    """
    The CLIP Interrogator is a prompt engineering tool that combines OpenAI's CLIP and Salesforce's BLIP to optimize text prompts to match a given image. Use the resulting prompts with text-to-image models like Stable Diffusion to create cool art!
    """

    Mode: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.analyze.ClipInterrogator.Mode
    )
    Clip_model_name: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.analyze.ClipInterrogator.Clip_model_name
    )

    mode: nodetool.nodes.replicate.image.analyze.ClipInterrogator.Mode = Field(
        default=nodetool.nodes.replicate.image.analyze.ClipInterrogator.Mode("best"),
        description="Prompt mode (best takes 10-20 seconds, fast takes 1-2 seconds).",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image",
    )
    clip_model_name: (
        nodetool.nodes.replicate.image.analyze.ClipInterrogator.Clip_model_name
    ) = Field(
        default=nodetool.nodes.replicate.image.analyze.ClipInterrogator.Clip_model_name(
            "ViT-L-14/openai"
        ),
        description="Choose ViT-L for Stable Diffusion 1, ViT-H for Stable Diffusion 2, or ViT-bigG for Stable Diffusion XL.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.ClipInterrogator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class Img2Prompt(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Get an approximate text prompt, with style, matching an image.  (Optimized for stable-diffusion (clip ViT-L/14))
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.Img2Prompt

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class Llava13b(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Visual instruction tuning towards large language and vision models with GPT-4 level capabilities
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image",
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=1,
        description="When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt to use for text generation"
    )
    max_tokens: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Maximum number of tokens to generate. A word is generally 2-3 tokens",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.2,
        description="Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.Llava13b

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class Moondream2(SingleOutputGraphNode[str], GraphNode[str]):
    """
    moondream2 is a small vision language model designed to run efficiently on edge devices
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Describe this image", description="Input prompt"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.Moondream2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class NSFWImageDetection(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Fine-Tuned Vision Transformer (ViT) for NSFW Image Classification
    """

    image: str | OutputHandle[str] | None = connect_field(
        default=None, description="Input image"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.NSFWImageDetection

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.image.analyze
from nodetool.workflows.base_node import BaseNode


class SDXLClipInterrogator(SingleOutputGraphNode[str], GraphNode[str]):
    """
    CLIP Interrogator for SDXL optimizes text prompts to match a given image
    """

    Mode: typing.ClassVar[type] = (
        nodetool.nodes.replicate.image.analyze.SDXLClipInterrogator.Mode
    )

    mode: nodetool.nodes.replicate.image.analyze.SDXLClipInterrogator.Mode = Field(
        default=nodetool.nodes.replicate.image.analyze.SDXLClipInterrogator.Mode(
            "best"
        ),
        description="Prompt Mode: fast takes 1-2 seconds, best takes 15-25 seconds.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.image.analyze.SDXLClipInterrogator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
