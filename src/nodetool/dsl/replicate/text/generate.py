# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Claude_3_7_Sonnet(SingleOutputGraphNode[str], GraphNode[str]):
    """
    The most intelligent Claude model and the first hybrid reasoning model on the market (claude-3-7-sonnet-20250219)
    """

    image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Optional input image. Images are priced as (width px * height px)/750 input tokens",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Input prompt"
    )
    max_tokens: int | OutputHandle[int] = connect_field(
        default=8192, description="Maximum number of output tokens"
    )
    system_prompt: str | OutputHandle[str] = connect_field(
        default="", description="System prompt"
    )
    max_image_resolution: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Maximum image resolution in megapixels. Scales down image before sending it to Claude, to save time and money.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Claude_3_7_Sonnet

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Deepseek_R1(SingleOutputGraphNode[str], GraphNode[str]):
    """
    A reasoning model trained with reinforcement learning, on par with OpenAI o1
    """

    top_p: float | OutputHandle[float] = connect_field(
        default=1, description="Top-p (nucleus) sampling"
    )
    prompt: str | OutputHandle[str] = connect_field(default="", description="Prompt")
    max_tokens: int | OutputHandle[int] = connect_field(
        default=20480,
        description="The maximum number of tokens the model should generate as output.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.1,
        description="The value used to modulate the next token probabilities.",
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=0, description="Presence penalty"
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0, description="Frequency penalty"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Deepseek_R1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Deepseek_V3_1(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Latest hybrid thinking model from Deepseek
    """

    top_p: float | OutputHandle[float] = connect_field(
        default=1, description="Top-p (nucleus) sampling"
    )
    prompt: str | OutputHandle[str] = connect_field(default="", description="Prompt")
    max_tokens: int | OutputHandle[int] = connect_field(
        default=1024,
        description="The maximum number of tokens the model should generate as output.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.1,
        description="The value used to modulate the next token probabilities.",
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=0, description="Presence penalty"
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0, description="Frequency penalty"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Deepseek_V3_1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class GPT_4_1(SingleOutputGraphNode[str], GraphNode[str]):
    """
    OpenAI's Flagship GPT model for complex tasks.
    """

    top_p: float | OutputHandle[float] = connect_field(
        default=1,
        description="Nucleus sampling parameter - the model considers the results of the tokens with top_p probability mass. (0.1 means only the tokens comprising the top 10% probability mass are considered.)",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The prompt to send to the model. Do not use if using messages.",
    )
    messages: list | OutputHandle[list] = connect_field(
        default=[],
        description='A JSON string representing a list of messages. For example: [{"role": "user", "content": "Hello, how are you?"}]. If provided, prompt and system_prompt are ignored.',
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[], description="List of images to send to the model"
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=1, description="Sampling temperature between 0 and 2"
    )
    system_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="System prompt to set the assistant's behavior"
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=0,
        description="Presence penalty parameter - positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0,
        description="Frequency penalty parameter - positive values penalize the repetition of tokens.",
    )
    max_completion_tokens: int | OutputHandle[int] = connect_field(
        default=4096, description="Maximum number of completion tokens to generate"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.GPT_4_1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class GPT_4_1_Mini(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Fast, affordable version of GPT-4.1
    """

    top_p: float | OutputHandle[float] = connect_field(
        default=1,
        description="Nucleus sampling parameter - the model considers the results of the tokens with top_p probability mass. (0.1 means only the tokens comprising the top 10% probability mass are considered.)",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The prompt to send to the model. Do not use if using messages.",
    )
    messages: list | OutputHandle[list] = connect_field(
        default=[],
        description='A JSON string representing a list of messages. For example: [{"role": "user", "content": "Hello, how are you?"}]. If provided, prompt and system_prompt are ignored.',
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[], description="List of images to send to the model"
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=1, description="Sampling temperature between 0 and 2"
    )
    system_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="System prompt to set the assistant's behavior"
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=0,
        description="Presence penalty parameter - positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0,
        description="Frequency penalty parameter - positive values penalize the repetition of tokens.",
    )
    max_completion_tokens: int | OutputHandle[int] = connect_field(
        default=4096, description="Maximum number of completion tokens to generate"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.GPT_4_1_Mini

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class GPT_4_1_Nano(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Fastest, most cost-effective GPT-4.1 model from OpenAI
    """

    top_p: float | OutputHandle[float] = connect_field(
        default=1,
        description="Nucleus sampling parameter - the model considers the results of the tokens with top_p probability mass. (0.1 means only the tokens comprising the top 10% probability mass are considered.)",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The prompt to send to the model. Do not use if using messages.",
    )
    messages: list | OutputHandle[list] = connect_field(
        default=[],
        description='A JSON string representing a list of messages. For example: [{"role": "user", "content": "Hello, how are you?"}]. If provided, prompt and system_prompt are ignored.',
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[], description="List of images to send to the model"
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=1, description="Sampling temperature between 0 and 2"
    )
    system_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="System prompt to set the assistant's behavior"
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=0,
        description="Presence penalty parameter - positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0,
        description="Frequency penalty parameter - positive values penalize the repetition of tokens.",
    )
    max_completion_tokens: int | OutputHandle[int] = connect_field(
        default=4096, description="Maximum number of completion tokens to generate"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.GPT_4_1_Nano

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class GPT_5(SingleOutputGraphNode[str], GraphNode[str]):
    """
    OpenAI's new model excelling at coding, writing, and reasoning.
    """

    Verbosity: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5.Verbosity
    )
    Reasoning_effort: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5.Reasoning_effort
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The prompt to send to the model. Do not use if using messages.",
    )
    messages: list | OutputHandle[list] = connect_field(
        default=[],
        description='A JSON string representing a list of messages. For example: [{"role": "user", "content": "Hello, how are you?"}]. If provided, prompt and system_prompt are ignored.',
    )
    verbosity: nodetool.nodes.replicate.text.generate.GPT_5.Verbosity = Field(
        default=nodetool.nodes.replicate.text.generate.GPT_5.Verbosity("medium"),
        description="Constrains the verbosity of the model's response. Lower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are low, medium, and high. GPT-5 supports this parameter to help control whether answers are short and to the point or long and comprehensive.",
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[], description="List of images to send to the model"
    )
    system_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="System prompt to set the assistant's behavior"
    )
    reasoning_effort: nodetool.nodes.replicate.text.generate.GPT_5.Reasoning_effort = (
        Field(
            default=nodetool.nodes.replicate.text.generate.GPT_5.Reasoning_effort(
                "minimal"
            ),
            description="Constrains effort on reasoning for GPT-5 models. Currently supported values are minimal, low, medium, and high. The minimal value gets answers back faster without extensive reasoning first. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
        )
    )
    max_completion_tokens: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Maximum number of completion tokens to generate. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.GPT_5

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class GPT_5_Mini(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Faster version of OpenAI's flagship GPT-5 model
    """

    Verbosity: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5_Mini.Verbosity
    )
    Reasoning_effort: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5_Mini.Reasoning_effort
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The prompt to send to the model. Do not use if using messages.",
    )
    messages: list | OutputHandle[list] = connect_field(
        default=[],
        description='A JSON string representing a list of messages. For example: [{"role": "user", "content": "Hello, how are you?"}]. If provided, prompt and system_prompt are ignored.',
    )
    verbosity: nodetool.nodes.replicate.text.generate.GPT_5_Mini.Verbosity = Field(
        default=nodetool.nodes.replicate.text.generate.GPT_5_Mini.Verbosity("medium"),
        description="Constrains the verbosity of the model's response. Lower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are low, medium, and high. GPT-5 supports this parameter to help control whether answers are short and to the point or long and comprehensive.",
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[], description="List of images to send to the model"
    )
    system_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="System prompt to set the assistant's behavior"
    )
    reasoning_effort: (
        nodetool.nodes.replicate.text.generate.GPT_5_Mini.Reasoning_effort
    ) = Field(
        default=nodetool.nodes.replicate.text.generate.GPT_5_Mini.Reasoning_effort(
            "minimal"
        ),
        description="Constrains effort on reasoning for GPT-5 models. Currently supported values are minimal, low, medium, and high. The minimal value gets answers back faster without extensive reasoning first. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
    )
    max_completion_tokens: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Maximum number of completion tokens to generate. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.GPT_5_Mini

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class GPT_5_Nano(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Fastest, most cost-effective GPT-5 model from OpenAI
    """

    Verbosity: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5_Nano.Verbosity
    )
    Reasoning_effort: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5_Nano.Reasoning_effort
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The prompt to send to the model. Do not use if using messages.",
    )
    messages: list | OutputHandle[list] = connect_field(
        default=[],
        description='A JSON string representing a list of messages. For example: [{"role": "user", "content": "Hello, how are you?"}]. If provided, prompt and system_prompt are ignored.',
    )
    verbosity: nodetool.nodes.replicate.text.generate.GPT_5_Nano.Verbosity = Field(
        default=nodetool.nodes.replicate.text.generate.GPT_5_Nano.Verbosity("medium"),
        description="Constrains the verbosity of the model's response. Lower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are low, medium, and high. GPT-5 supports this parameter to help control whether answers are short and to the point or long and comprehensive.",
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[], description="List of images to send to the model"
    )
    system_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="System prompt to set the assistant's behavior"
    )
    reasoning_effort: (
        nodetool.nodes.replicate.text.generate.GPT_5_Nano.Reasoning_effort
    ) = Field(
        default=nodetool.nodes.replicate.text.generate.GPT_5_Nano.Reasoning_effort(
            "minimal"
        ),
        description="Constrains effort on reasoning for GPT-5 models. Currently supported values are minimal, low, medium, and high. The minimal value gets answers back faster without extensive reasoning first. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
    )
    max_completion_tokens: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Maximum number of completion tokens to generate. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.GPT_5_Nano

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class GPT_5_Structured(SingleOutputGraphNode[str], GraphNode[str]):
    """
    GPT-5 with support for structured outputs, web search and custom tools
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5_Structured.Model
    )
    Verbosity: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5_Structured.Verbosity
    )
    Reasoning_effort: typing.ClassVar[type] = (
        nodetool.nodes.replicate.text.generate.GPT_5_Structured.Reasoning_effort
    )

    model: nodetool.nodes.replicate.text.generate.GPT_5_Structured.Model = Field(
        default=nodetool.nodes.replicate.text.generate.GPT_5_Structured.Model("gpt-5"),
        description="GPT-5 model to use.",
    )
    tools: list | OutputHandle[list] = connect_field(
        default=[],
        description="Tools to make available to the model. Should be a JSON object containing a list of tool definitions.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="A simple text input to the model, equivalent to a text input with the user role. Ignored if input_item_list is provided.",
    )
    verbosity: nodetool.nodes.replicate.text.generate.GPT_5_Structured.Verbosity = (
        Field(
            default=nodetool.nodes.replicate.text.generate.GPT_5_Structured.Verbosity(
                "medium"
            ),
            description="Constrains the verbosity of the model's response. Lower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are low, medium, and high. GPT-5 supports this parameter to help control whether answers are short and to the point or long and comprehensive.",
        )
    )
    image_input: list | OutputHandle[list] = connect_field(
        default=[], description="List of images to send to the model"
    )
    json_schema: dict | OutputHandle[dict] = connect_field(
        default={},
        description="A JSON schema that the response must conform to. For simple data structures we recommend using `simple_text_format_schema` which will be converted to a JSON schema for you.",
    )
    instructions: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="A system (or developer) message inserted into the model's context. When using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses.",
    )
    simple_schema: list | OutputHandle[list] = connect_field(
        default=[],
        description="Create a JSON schema for the output to conform to. The schema will be created from a simple list of field specifications. Strings: 'thing' (defaults to string), 'thing:str', 'thing:string'. Booleans: 'is_a_thing:bool' or 'is_a_thing:boolean'. Numbers: 'count:number', 'count:int'. Lists: 'things:list' (defaults to list of strings), 'things:list:str', 'number_things:list:number', etc. Nested objects are not supported, use `json_schema` instead.",
    )
    input_item_list: list | OutputHandle[list] = connect_field(
        default=[],
        description="A list of one or many input items to the model, containing different content types. This parameter corresponds with the `input` OpenAI API parameter. For more details see: https://platform.openai.com/docs/api-reference/responses/create#responses_create-input. Similar to the `messages` parameter, but with more flexibility in the content types.",
    )
    reasoning_effort: (
        nodetool.nodes.replicate.text.generate.GPT_5_Structured.Reasoning_effort
    ) = Field(
        default=nodetool.nodes.replicate.text.generate.GPT_5_Structured.Reasoning_effort(
            "minimal"
        ),
        description="Constrains effort on reasoning for GPT-5 models. Currently supported values are minimal, low, medium, and high. The minimal value gets answers back faster without extensive reasoning first. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
    )
    enable_web_search: bool | OutputHandle[bool] = connect_field(
        default=False, description="Allow GPT-5 to use web search for the response."
    )
    max_output_tokens: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Maximum number of completion tokens to generate. For higher reasoning efforts you may need to increase your max_completion_tokens to avoid empty responses (where all the tokens are used on reasoning).",
    )
    previous_response_id: str | OutputHandle[str] | None = connect_field(
        default=None, description="The ID of a previous response to continue from."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.GPT_5_Structured

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Llama3_1_405B_Instruct(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Meta's flagship 405 billion parameter language model, fine-tuned for chat completions
    """

    top_k: int | OutputHandle[int] = connect_field(
        default=50,
        description="The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering).",
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=0.9,
        description="A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).",
    )
    prompt: str | OutputHandle[str] = connect_field(default="", description="Prompt")
    max_tokens: int | OutputHandle[int] = connect_field(
        default=512,
        description="The maximum number of tokens the model should generate as output.",
    )
    min_tokens: int | OutputHandle[int] = connect_field(
        default=0,
        description="The minimum number of tokens the model should generate as output.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="The value used to modulate the next token probabilities.",
    )
    system_prompt: str | OutputHandle[str] = connect_field(
        default="You are a helpful assistant.",
        description="System prompt to send to the model. This is prepended to the prompt and helps guide system behavior. Ignored for non-chat models.",
    )
    stop_sequences: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="A comma-separated list of sequences to stop generation at. For example, '<end>,<stop>' will stop generation at the first instance of 'end' or '<stop>'.",
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=0, description="Presence penalty"
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0, description="Frequency penalty"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Llama3_1_405B_Instruct

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Llama3_70B(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Base version of Llama 3, a 70 billion parameter language model from Meta.
    """

    top_k: int | OutputHandle[int] = connect_field(
        default=50,
        description="The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering).",
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=0.9,
        description="A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).",
    )
    prompt: str | OutputHandle[str] = connect_field(default="", description="Prompt")
    max_tokens: int | OutputHandle[int] = connect_field(
        default=512,
        description="The maximum number of tokens the model should generate as output.",
    )
    min_tokens: int | OutputHandle[int] = connect_field(
        default=0,
        description="The minimum number of tokens the model should generate as output.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="The value used to modulate the next token probabilities.",
    )
    prompt_template: str | OutputHandle[str] = connect_field(
        default="{prompt}",
        description="Prompt template. The string `{prompt}` will be substituted for the input prompt. If you want to generate dialog output, use this template as a starting point and construct the prompt string manually, leaving `prompt_template={prompt}`.",
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=1.15, description="Presence penalty"
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0.2, description="Frequency penalty"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Llama3_70B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Llama3_70B_Instruct(SingleOutputGraphNode[str], GraphNode[str]):
    """
    A 70 billion parameter language model from Meta, fine tuned for chat completions
    """

    top_k: int | OutputHandle[int] = connect_field(
        default=50,
        description="The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering).",
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=0.9,
        description="A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).",
    )
    prompt: str | OutputHandle[str] = connect_field(default="", description="Prompt")
    max_tokens: int | OutputHandle[int] = connect_field(
        default=512,
        description="The maximum number of tokens the model should generate as output.",
    )
    min_tokens: int | OutputHandle[int] = connect_field(
        default=0,
        description="The minimum number of tokens the model should generate as output.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="The value used to modulate the next token probabilities.",
    )
    prompt_template: str | OutputHandle[str] = connect_field(
        default="{prompt}",
        description="Prompt template. The string `{prompt}` will be substituted for the input prompt. If you want to generate dialog output, use this template as a starting point and construct the prompt string manually, leaving `prompt_template={prompt}`.",
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=1.15, description="Presence penalty"
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0.2, description="Frequency penalty"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Llama3_70B_Instruct

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Llama3_8B(SingleOutputGraphNode[str], GraphNode[str]):
    """
    Base version of Llama 3, an 8 billion parameter language model from Meta.
    """

    top_k: int | OutputHandle[int] = connect_field(
        default=50,
        description="The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering).",
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=0.9,
        description="A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).",
    )
    prompt: str | OutputHandle[str] = connect_field(default="", description="Prompt")
    max_tokens: int | OutputHandle[int] = connect_field(
        default=512,
        description="The maximum number of tokens the model should generate as output.",
    )
    min_tokens: int | OutputHandle[int] = connect_field(
        default=0,
        description="The minimum number of tokens the model should generate as output.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="The value used to modulate the next token probabilities.",
    )
    prompt_template: str | OutputHandle[str] = connect_field(
        default="{prompt}",
        description="Prompt template. The string `{prompt}` will be substituted for the input prompt. If you want to generate dialog output, use this template as a starting point and construct the prompt string manually, leaving `prompt_template={prompt}`.",
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=1.15, description="Presence penalty"
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0.2, description="Frequency penalty"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Llama3_8B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Llama3_8B_Instruct(SingleOutputGraphNode[str], GraphNode[str]):
    """
    An 8 billion parameter language model from Meta, fine tuned for chat completions
    """

    top_k: int | OutputHandle[int] = connect_field(
        default=50,
        description="The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering).",
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=0.9,
        description="A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).",
    )
    prompt: str | OutputHandle[str] = connect_field(default="", description="Prompt")
    max_tokens: int | OutputHandle[int] = connect_field(
        default=512,
        description="The maximum number of tokens the model should generate as output.",
    )
    min_tokens: int | OutputHandle[int] = connect_field(
        default=0,
        description="The minimum number of tokens the model should generate as output.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.6,
        description="The value used to modulate the next token probabilities.",
    )
    prompt_template: str | OutputHandle[str] = connect_field(
        default="{prompt}",
        description="Prompt template. The string `{prompt}` will be substituted for the input prompt. If you want to generate dialog output, use this template as a starting point and construct the prompt string manually, leaving `prompt_template={prompt}`.",
    )
    presence_penalty: float | OutputHandle[float] = connect_field(
        default=1.15, description="Presence penalty"
    )
    frequency_penalty: float | OutputHandle[float] = connect_field(
        default=0.2, description="Frequency penalty"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Llama3_8B_Instruct

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class LlamaGuard_3_11B_Vision(SingleOutputGraphNode[str], GraphNode[str]):
    """
    A Llama-3.2-11B pretrained model, fine-tuned for content safety classification
    """

    image: str | OutputHandle[str] | None = connect_field(
        default=None, description="Image to moderate"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Which one should I buy?", description="User message to moderate"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.LlamaGuard_3_11B_Vision

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class LlamaGuard_3_8B(SingleOutputGraphNode[str], GraphNode[str]):
    """
    A Llama-3.1-8B pretrained model, fine-tuned for content safety classification
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="I forgot how to kill a process in Linux, can you help?",
        description="User message to moderate",
    )
    assistant: str | OutputHandle[str] | None = connect_field(
        default=None, description="Assistant response to classify"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.LlamaGuard_3_8B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.text.generate
from nodetool.workflows.base_node import BaseNode


class Snowflake_Arctic_Instruct(SingleOutputGraphNode[str], GraphNode[str]):
    """
    An efficient, intelligent, and truly open-source language model
    """

    name: str | OutputHandle[str] | None = connect_field(default=None, description=None)
    name_file: str | OutputHandle[str] | None = connect_field(
        default=None, description=None
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.text.generate.Snowflake_Arctic_Instruct

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
