# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.transcribe
from nodetool.workflows.base_node import BaseNode


class GPT4o_Transcribe(SingleOutputGraphNode[str], GraphNode[str]):
    """
    A speech-to-text model that uses GPT-4o to transcribe audio
    """

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.",
    )
    language: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The language of the input audio. Supplying the input language in ISO-639-1 (e.g. en) format will improve accuracy and latency.",
    )
    audio_file: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0, description="Sampling temperature between 0 and 1"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.transcribe.GPT4o_Transcribe

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.transcribe
from nodetool.workflows.base_node import BaseNode


class IncrediblyFastWhisper(SingleOutputGraphNode[str], GraphNode[str]):
    """
    whisper-large-v3, incredibly fast, powered by Hugging Face Transformers! ðŸ¤—
    """

    Task: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Task
    )
    Language: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Language
    )
    Timestamp: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Timestamp
    )

    task: nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Task = Field(
        default=nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Task(
            "transcribe"
        ),
        description="Task to perform: transcribe or translate to another language.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Audio file",
    )
    hf_token: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Provide a hf.co/settings/token for Pyannote.audio to diarise the audio clips. You need to agree to the terms in 'https://huggingface.co/pyannote/speaker-diarization-3.1' and 'https://huggingface.co/pyannote/segmentation-3.0' first.",
    )
    language: (
        nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Language
    ) = Field(
        default=nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Language(
            "None"
        ),
        description="Language spoken in the audio, specify 'None' to perform language detection.",
    )
    timestamp: (
        nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Timestamp
    ) = Field(
        default=nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper.Timestamp(
            "chunk"
        ),
        description="Whisper supports both chunked as well as word level timestamps.",
    )
    batch_size: int | OutputHandle[int] = connect_field(
        default=24,
        description="Number of parallel batches you want to compute. Reduce if you face OOMs.",
    )
    diarise_audio: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Use Pyannote.audio to diarise the audio clips. You will need to provide hf_token below too.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.transcribe.IncrediblyFastWhisper

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
