# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.generate
from nodetool.workflows.base_node import BaseNode


class MMAudio(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Add sound to video using the MMAudio V2 model. An advanced AI model that synthesizes high-quality audio from video content, enabling seamless video-to-audio transformation.
    """

    seed: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Random seed. Use -1 or leave blank to randomize the seed",
    )
    image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Optional image file for image-to-audio generation (experimental)",
    )
    video: str | OutputHandle[str] | None = connect_field(
        default=None, description="Optional video file for video-to-audio generation"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for generated audio"
    )
    duration: float | OutputHandle[float] = connect_field(
        default=8, description="Duration of output in seconds"
    )
    num_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps"
    )
    cfg_strength: float | OutputHandle[float] = connect_field(
        default=4.5, description="Guidance strength (CFG)"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="music", description="Negative prompt to avoid certain sounds"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.generate.MMAudio

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.generate
from nodetool.workflows.base_node import BaseNode


class MusicGen(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Generate music from a prompt or melody
    """

    Model_version: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.MusicGen.Model_version
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.MusicGen.Output_format
    )
    Normalization_strategy: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.MusicGen.Normalization_strategy
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Seed for random number generator. If None or -1, a random seed will be used.",
    )
    top_k: int | OutputHandle[int] = connect_field(
        default=250, description="Reduces sampling to the k most likely tokens."
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=0,
        description="Reduces sampling to tokens with cumulative probability of p. When set to  `0` (default), top_k sampling is used.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="A description of the music you want to generate."
    )
    duration: int | OutputHandle[int] = connect_field(
        default=8, description="Duration of the generated audio in seconds."
    )
    input_audio: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="An audio file that will influence the generated music. If `continuation` is `True`, the generated music will be a continuation of the audio file. Otherwise, the generated music will mimic the audio file's melody.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=1,
        description="Controls the 'conservativeness' of the sampling process. Higher temperature means more diversity.",
    )
    continuation: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, generated music will continue from `input_audio`. Otherwise, generated music will mimic `input_audio`'s melody.",
    )
    model_version: nodetool.nodes.replicate.audio.generate.MusicGen.Model_version = (
        Field(
            default=nodetool.nodes.replicate.audio.generate.MusicGen.Model_version(
                "stereo-melody-large"
            ),
            description="Model to use for generation",
        )
    )
    output_format: nodetool.nodes.replicate.audio.generate.MusicGen.Output_format = (
        Field(
            default=nodetool.nodes.replicate.audio.generate.MusicGen.Output_format(
                "wav"
            ),
            description="Output format for generated audio.",
        )
    )
    continuation_end: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="End time of the audio file to use for continuation. If -1 or None, will default to the end of the audio clip.",
    )
    continuation_start: int | OutputHandle[int] = connect_field(
        default=0, description="Start time of the audio file to use for continuation."
    )
    multi_band_diffusion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the EnCodec tokens will be decoded with MultiBand Diffusion. Only works with non-stereo models.",
    )
    normalization_strategy: (
        nodetool.nodes.replicate.audio.generate.MusicGen.Normalization_strategy
    ) = Field(
        default=nodetool.nodes.replicate.audio.generate.MusicGen.Normalization_strategy(
            "loudness"
        ),
        description="Strategy for normalizing audio.",
    )
    classifier_free_guidance: int | OutputHandle[int] = connect_field(
        default=3,
        description="Increases the influence of inputs on the output. Higher values produce lower-varience outputs that adhere more closely to inputs.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.generate.MusicGen

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.generate
from nodetool.workflows.base_node import BaseNode


class RealisticVoiceCloning(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """
    Create song covers with any RVC v2 trained AI voice from audio files.
    """

    Rvc_model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Rvc_model
    )
    Pitch_change: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Pitch_change
    )
    Output_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Output_format
    )
    Pitch_detection_algorithm: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Pitch_detection_algorithm
    )

    protect: float | OutputHandle[float] = connect_field(
        default=0.33,
        description="Control how much of the original vocals' breath and voiceless consonants to leave in the AI vocals. Set 0.5 to disable.",
    )
    rvc_model: (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Rvc_model
    ) = Field(
        default=nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Rvc_model(
            "Squidward"
        ),
        description="RVC model for a specific voice. If using a custom model, this should match the name of the downloaded model. If a 'custom_rvc_model_download_url' is provided, this will be automatically set to the name of the downloaded model.",
    )
    index_rate: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Control how much of the AI's accent to leave in the vocals.",
    )
    song_input: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Upload your audio file here.",
    )
    reverb_size: float | OutputHandle[float] = connect_field(
        default=0.15, description="The larger the room, the longer the reverb time."
    )
    pitch_change: (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Pitch_change
    ) = Field(
        default=nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Pitch_change(
            "no-change"
        ),
        description="Adjust pitch of AI vocals. Options: `no-change`, `male-to-female`, `female-to-male`.",
    )
    rms_mix_rate: float | OutputHandle[float] = connect_field(
        default=0.25,
        description="Control how much to use the original vocal's loudness (0) or a fixed loudness (1).",
    )
    filter_radius: int | OutputHandle[int] = connect_field(
        default=3,
        description="If >=3: apply median filtering median filtering to the harvested pitch results.",
    )
    output_format: (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Output_format
    ) = Field(
        default=nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Output_format(
            "mp3"
        ),
        description="wav for best quality and large file size, mp3 for decent quality and small file size.",
    )
    reverb_damping: float | OutputHandle[float] = connect_field(
        default=0.7, description="Absorption of high frequencies in the reverb."
    )
    reverb_dryness: float | OutputHandle[float] = connect_field(
        default=0.8, description="Level of AI vocals without reverb."
    )
    reverb_wetness: float | OutputHandle[float] = connect_field(
        default=0.2, description="Level of AI vocals with reverb."
    )
    crepe_hop_length: int | OutputHandle[int] = connect_field(
        default=128,
        description="When `pitch_detection_algo` is set to `mangio-crepe`, this controls how often it checks for pitch changes in milliseconds. Lower values lead to longer conversions and higher risk of voice cracks, but better pitch accuracy.",
    )
    pitch_change_all: float | OutputHandle[float] = connect_field(
        default=0,
        description="Change pitch/key of background music, backup vocals and AI vocals in semitones. Reduces sound quality slightly.",
    )
    main_vocals_volume_change: float | OutputHandle[float] = connect_field(
        default=0,
        description="Control volume of main AI vocals. Use -3 to decrease the volume by 3 decibels, or 3 to increase the volume by 3 decibels.",
    )
    pitch_detection_algorithm: (
        nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Pitch_detection_algorithm
    ) = Field(
        default=nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning.Pitch_detection_algorithm(
            "rmvpe"
        ),
        description="Best option is rmvpe (clarity in vocals), then mangio-crepe (smoother vocals).",
    )
    instrumental_volume_change: float | OutputHandle[float] = connect_field(
        default=0, description="Control volume of the background music/instrumentals."
    )
    backup_vocals_volume_change: float | OutputHandle[float] = connect_field(
        default=0, description="Control volume of backup AI vocals."
    )
    custom_rvc_model_download_url: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="URL to download a custom RVC model. If provided, the model will be downloaded (if it doesn't already exist) and used for prediction, regardless of the 'rvc_model' value.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.generate.RealisticVoiceCloning

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.generate
from nodetool.workflows.base_node import BaseNode


class Riffusion(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Stable diffusion for real-time music generation
    """

    Seed_image_id: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.Riffusion.Seed_image_id
    )

    alpha: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Interpolation alpha if using two prompts. A value of 0 uses prompt_a fully, a value of 1 uses prompt_b fully",
    )
    prompt_a: str | OutputHandle[str] = connect_field(
        default="funky synth solo", description="The prompt for your audio"
    )
    prompt_b: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="The second prompt to interpolate with the first, leave blank if no interpolation",
    )
    denoising: float | OutputHandle[float] = connect_field(
        default=0.75, description="How much to transform input spectrogram"
    )
    seed_image_id: nodetool.nodes.replicate.audio.generate.Riffusion.Seed_image_id = (
        Field(
            default=nodetool.nodes.replicate.audio.generate.Riffusion.Seed_image_id(
                "vibes"
            ),
            description="Seed spectrogram to use",
        )
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of steps to run the diffusion model"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.generate.Riffusion

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.generate
from nodetool.workflows.base_node import BaseNode


class Speech_02_HD(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Text-to-Audio (T2A) that offers voice synthesis, emotional expression, and multilingual capabilities. Optimized for high-fidelity applications like voiceovers and audiobooks.
    """

    Bitrate: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.Speech_02_HD.Bitrate
    )
    Channel: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.Speech_02_HD.Channel
    )
    Emotion: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.Speech_02_HD.Emotion
    )
    Sample_rate: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.Speech_02_HD.Sample_rate
    )
    Audio_format: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.Speech_02_HD.Audio_format
    )
    Language_boost: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.Speech_02_HD.Language_boost
    )

    text: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Text to narrate (max 10,000 characters). Use markers like <#0.5#> to insert pauses in seconds.",
    )
    pitch: int | OutputHandle[int] = connect_field(
        default=0, description="Semitone offset applied to the voice (−12 to +12)."
    )
    speed: float | OutputHandle[float] = connect_field(
        default=1,
        description="Speech speed multiplier (0.5–2.0). Lower is slower, higher is faster.",
    )
    volume: float | OutputHandle[float] = connect_field(
        default=1,
        description="Relative loudness. 1.0 is default MiniMax gain. Range 0–10.",
    )
    bitrate: nodetool.nodes.replicate.audio.generate.Speech_02_HD.Bitrate = Field(
        default=nodetool.nodes.replicate.audio.generate.Speech_02_HD.Bitrate(128000),
        description="MP3 bitrate in bits per second. Only used when audio_format is mp3.",
    )
    channel: nodetool.nodes.replicate.audio.generate.Speech_02_HD.Channel = Field(
        default=nodetool.nodes.replicate.audio.generate.Speech_02_HD.Channel("mono"),
        description="mono for 1 channel (default), stereo for 2 channels.",
    )
    emotion: nodetool.nodes.replicate.audio.generate.Speech_02_HD.Emotion = Field(
        default=nodetool.nodes.replicate.audio.generate.Speech_02_HD.Emotion("auto"),
        description="Desired delivery style. Use auto to let MiniMax choose, or pick a specific emotion.",
    )
    voice_id: str | OutputHandle[str] = connect_field(
        default="Wise_Woman",
        description="Voice to synthesize. Pick any MiniMax system voice or a voice_id returned by https://replicate.com/minimax/voice-cloning.",
    )
    sample_rate: nodetool.nodes.replicate.audio.generate.Speech_02_HD.Sample_rate = (
        Field(
            default=nodetool.nodes.replicate.audio.generate.Speech_02_HD.Sample_rate(
                32000
            ),
            description="Audio sample rate in Hz.",
        )
    )
    audio_format: nodetool.nodes.replicate.audio.generate.Speech_02_HD.Audio_format = (
        Field(
            default=nodetool.nodes.replicate.audio.generate.Speech_02_HD.Audio_format(
                "mp3"
            ),
            description="File format for the generated audio. Choose mp3 for general use, wav/flac for lossless, or pcm for raw bytes.",
        )
    )
    language_boost: (
        nodetool.nodes.replicate.audio.generate.Speech_02_HD.Language_boost
    ) = Field(
        default=nodetool.nodes.replicate.audio.generate.Speech_02_HD.Language_boost(
            "None"
        ),
        description="Optional language hint. Choose Automatic to let MiniMax detect the language, or pick a specific locale.",
    )
    subtitle_enable: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Return MiniMax subtitle metadata with sentence timestamps (non-streaming only).",
    )
    english_normalization: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Improve number/date reading for English text (adds a small amount of latency).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.generate.Speech_02_HD

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.generate
from nodetool.workflows.base_node import BaseNode


class StyleTTS2(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Generates speech from text
    """

    beta: float | OutputHandle[float] = connect_field(
        default=0.7,
        description="Only used for long text inputs or in case of reference speaker,             determines the prosody of the speaker. Use lower values to sample style based             on previous or reference speech instead of text.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=0, description="Seed for reproducibility"
    )
    text: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text to convert to speech"
    )
    alpha: float | OutputHandle[float] = connect_field(
        default=0.3,
        description="Only used for long text inputs or in case of reference speaker,             determines the timbre of the speaker. Use lower values to sample style based             on previous or reference speech instead of text.",
    )
    weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Replicate weights url for inference with model that is fine-tuned on new speakers.            If provided, a reference speech must also be provided.             If not provided, the default model will be used.",
    )
    reference: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference speech to copy style from",
    )
    diffusion_steps: int | OutputHandle[int] = connect_field(
        default=10, description="Number of diffusion steps"
    )
    embedding_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Embedding scale, use higher values for pronounced emotion",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.generate.StyleTTS2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.audio.generate
from nodetool.workflows.base_node import BaseNode


class TortoiseTTS(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Generate speech from text, clone voices from mp3 files. From James Betker AKA "neonbjb".
    """

    Preset: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.TortoiseTTS.Preset
    )
    Voice_a: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_a
    )
    Voice_b: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_b
    )
    Voice_c: typing.ClassVar[type] = (
        nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_c
    )

    seed: int | OutputHandle[int] = connect_field(
        default=0, description="Random seed which can be used to reproduce results."
    )
    text: str | OutputHandle[str] = connect_field(
        default="The expressiveness of autoregressive transformers is literally nuts! I absolutely adore them.",
        description="Text to speak.",
    )
    preset: nodetool.nodes.replicate.audio.generate.TortoiseTTS.Preset = Field(
        default=nodetool.nodes.replicate.audio.generate.TortoiseTTS.Preset("fast"),
        description="Which voice preset to use. See the documentation for more information.",
    )
    voice_a: nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_a = Field(
        default=nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_a("random"),
        description="Selects the voice to use for generation. Use `random` to select a random voice. Use `custom_voice` to use a custom voice.",
    )
    voice_b: nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_b = Field(
        default=nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_b("disabled"),
        description="(Optional) Create new voice from averaging the latents for `voice_a`, `voice_b` and `voice_c`. Use `disabled` to disable voice mixing.",
    )
    voice_c: nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_c = Field(
        default=nodetool.nodes.replicate.audio.generate.TortoiseTTS.Voice_c("disabled"),
        description="(Optional) Create new voice from averaging the latents for `voice_a`, `voice_b` and `voice_c`. Use `disabled` to disable voice mixing.",
    )
    cvvp_amount: float | OutputHandle[float] = connect_field(
        default=0,
        description="How much the CVVP model should influence the output. Increasing this can in some cases reduce the likelyhood of multiple speakers. Defaults to 0 (disabled)",
    )
    custom_voice: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="(Optional) Create a custom voice based on an mp3 file of a speaker. Audio should be at least 15 seconds, only contain one speaker, and be in mp3 format. Overrides the `voice_a` input.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.audio.generate.TortoiseTTS

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
