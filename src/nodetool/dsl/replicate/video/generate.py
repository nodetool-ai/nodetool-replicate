# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class AudioToWaveform(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Create a waveform video from audio
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Audio file to create waveform from",
    )
    bg_color: str | OutputHandle[str] = connect_field(
        default="#000000", description="Background color of waveform"
    )
    fg_alpha: float | OutputHandle[float] = connect_field(
        default=0.75, description="Opacity of foreground waveform"
    )
    bar_count: int | OutputHandle[int] = connect_field(
        default=100, description="Number of bars in waveform"
    )
    bar_width: float | OutputHandle[float] = connect_field(
        default=0.4,
        description="Width of bars in waveform. 1 represents full width, 0.5 represents half width, etc.",
    )
    bars_color: str | OutputHandle[str] = connect_field(
        default="#ffffff", description="Color of waveform bars"
    )
    caption_text: str | OutputHandle[str] = connect_field(
        default="", description="Caption text for the video"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.AudioToWaveform

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Gen4_Aleph(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    A new way to edit, transform and generate video
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Gen4_Aleph.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    video: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Input video to generate from. Videos must be less than 16MB. Only 5s of the input video will be used.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for video generation"
    )
    aspect_ratio: nodetool.nodes.replicate.video.generate.Gen4_Aleph.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.video.generate.Gen4_Aleph.Aspect_ratio(
                "16:9"
            ),
            description="Video aspect ratio",
        )
    )
    reference_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Reference image to influence the style or content of the output.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Gen4_Aleph

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Gen4_Turbo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate 5s and 10s 720p videos fast
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Gen4_Turbo.Duration
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Gen4_Turbo.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: str | OutputHandle[str] | None = connect_field(
        default=None, description="Initial image for video generation (first frame)"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for video generation"
    )
    duration: nodetool.nodes.replicate.video.generate.Gen4_Turbo.Duration = Field(
        default=nodetool.nodes.replicate.video.generate.Gen4_Turbo.Duration(5),
        description="Duration of the output video in seconds",
    )
    aspect_ratio: nodetool.nodes.replicate.video.generate.Gen4_Turbo.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.video.generate.Gen4_Turbo.Aspect_ratio(
                "16:9"
            ),
            description="Video aspect ratio",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Gen4_Turbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Hailuo_02(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Hailuo 2 is a text-to-video and image-to-video model that can make 6s or 10s videos at 768p (standard) or 1080p (pro). It excels at real world physics.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Hailuo_02.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Hailuo_02.Resolution
    )

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for generation"
    )
    duration: nodetool.nodes.replicate.video.generate.Hailuo_02.Duration = Field(
        default=nodetool.nodes.replicate.video.generate.Hailuo_02.Duration(6),
        description="Duration of the video in seconds. 10 seconds is only available for 768p resolution.",
    )
    resolution: nodetool.nodes.replicate.video.generate.Hailuo_02.Resolution = Field(
        default=nodetool.nodes.replicate.video.generate.Hailuo_02.Resolution("1080p"),
        description="Pick between standard 512p, 768p, or pro 1080p resolution. The pro model is not just high resolution, it is also higher quality.",
    )
    last_frame_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Last frame image for video generation. The final frame of the output video will match this image.",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use prompt optimizer"
    )
    first_frame_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="First frame image for video generation. The output video will have the same aspect ratio as this image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Hailuo_02

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class HotshotXL(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    ðŸ˜Š Hotshot-XL is an AI text-to-GIF model trained to work alongside Stable Diffusion XL
    """

    Width: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.HotshotXL.Width
    )
    Height: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.HotshotXL.Height
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.HotshotXL.Scheduler
    )

    mp4: bool | OutputHandle[bool] = connect_field(
        default=False, description="Save as mp4, False for GIF"
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of denoising steps"
    )
    width: nodetool.nodes.replicate.video.generate.HotshotXL.Width = Field(
        default=nodetool.nodes.replicate.video.generate.HotshotXL.Width(672),
        description="Width of the output",
    )
    height: nodetool.nodes.replicate.video.generate.HotshotXL.Height = Field(
        default=nodetool.nodes.replicate.video.generate.HotshotXL.Height(384),
        description="Height of the output",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="a camel smoking a cigarette, hd, high quality",
        description="Input prompt",
    )
    scheduler: nodetool.nodes.replicate.video.generate.HotshotXL.Scheduler = Field(
        default=nodetool.nodes.replicate.video.generate.HotshotXL.Scheduler(
            "EulerAncestralDiscreteScheduler"
        ),
        description="Select a Scheduler",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry", description="Negative prompt"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.HotshotXL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Hunyuan_Video(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    A state-of-the-art text-to-video generation model capable of creating high-quality videos with realistic motion from text descriptions
    """

    fps: int | OutputHandle[int] = connect_field(
        default=24, description="Frames per second of the output video"
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed (leave empty for random)"
    )
    width: int | OutputHandle[int] = connect_field(
        default=864,
        description="Width of the video in pixels (must be divisible by 16)",
    )
    height: int | OutputHandle[int] = connect_field(
        default=480,
        description="Height of the video in pixels (must be divisible by 16)",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cat walks on the grass, realistic style",
        description="The prompt to guide the video generation",
    )
    infer_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )
    video_length: int | OutputHandle[int] = connect_field(
        default=129,
        description="Number of frames to generate (must be 4k+1, ex: 49 or 129)",
    )
    embedded_guidance_scale: float | OutputHandle[float] = connect_field(
        default=6, description="Guidance scale"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Hunyuan_Video

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Kling_Lip_Sync(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Add lip-sync to any video with an audio file or text
    """

    Voice_id: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Kling_Lip_Sync.Voice_id
    )

    text: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text content for lip sync (if not using audio)"
    )
    video_id: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="ID of a video generated by Kling. Cannot be used with video_url.",
    )
    voice_id: nodetool.nodes.replicate.video.generate.Kling_Lip_Sync.Voice_id = Field(
        default=nodetool.nodes.replicate.video.generate.Kling_Lip_Sync.Voice_id(
            "en_AOT"
        ),
        description="Voice ID for speech synthesis (if using text and not audio)",
    )
    video_url: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="URL of a video for lip syncing. It can be an .mp4 or .mov file, should be less than 100MB, with a duration of 2-10 seconds, and a resolution of 720p-1080p (720-1920px dimensions). Cannot be used with video_id.",
    )
    audio_file: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Audio file for lip sync. Must be .mp3, .wav, .m4a, or .aac and less than 5MB.",
    )
    voice_speed: float | OutputHandle[float] = connect_field(
        default=1, description="Speech rate (only used if using text and not audio)"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Kling_Lip_Sync

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Kling_V2_1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Use Kling v2.1 to generate 5s and 10s videos in 720p and 1080p resolution from a starting image (image-to-video)
    """

    Mode: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Kling_V2_1.Mode
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Kling_V2_1.Duration
    )

    mode: nodetool.nodes.replicate.video.generate.Kling_V2_1.Mode = Field(
        default=nodetool.nodes.replicate.video.generate.Kling_V2_1.Mode("standard"),
        description="Standard has a resolution of 720p, pro is 1080p. Both are 24fps.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for video generation"
    )
    duration: nodetool.nodes.replicate.video.generate.Kling_V2_1.Duration = Field(
        default=nodetool.nodes.replicate.video.generate.Kling_V2_1.Duration(5),
        description="Duration of the video in seconds",
    )
    end_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Last frame of the video (pro mode is required when this parameter is set)",
    )
    start_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="First frame of the video. You must use a start image with kling-v2.1.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Things you do not want to see in the video"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Kling_V2_1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class LTX_Video(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    LTX-Video is the first DiT-based video generation model capable of generating high-quality videos in real-time. It produces 24 FPS videos at a 768x512 resolution faster than they can be watched.
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.LTX_Video.Model
    )
    Length: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.LTX_Video.Length
    )
    Target_size: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.LTX_Video.Target_size
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.LTX_Video.Aspect_ratio
    )

    cfg: float | OutputHandle[float] = connect_field(
        default=3, description="How strongly the video follows the prompt"
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Set a seed for reproducibility. Random by default."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional input image to use as the starting frame",
    )
    model: nodetool.nodes.replicate.video.generate.LTX_Video.Model = Field(
        default=nodetool.nodes.replicate.video.generate.LTX_Video.Model("0.9.1"),
        description="Model version to use",
    )
    steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of steps"
    )
    length: nodetool.nodes.replicate.video.generate.LTX_Video.Length = Field(
        default=nodetool.nodes.replicate.video.generate.LTX_Video.Length(97),
        description="Length of the output video in frames",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="best quality, 4k, HDR, a tracking shot of a beautiful scene",
        description="Text prompt for the video. This model needs long descriptive prompts, if the prompt is too short the quality won't be good.",
    )
    target_size: nodetool.nodes.replicate.video.generate.LTX_Video.Target_size = Field(
        default=nodetool.nodes.replicate.video.generate.LTX_Video.Target_size(640),
        description="Target size for the output video",
    )
    aspect_ratio: nodetool.nodes.replicate.video.generate.LTX_Video.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.video.generate.LTX_Video.Aspect_ratio(
                "3:2"
            ),
            description="Aspect ratio of the output video. Ignored if an image is provided.",
        )
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, worst quality, deformed, distorted",
        description="Things you do not want to see in your video",
    )
    image_noise_scale: float | OutputHandle[float] = connect_field(
        default=0.15, description="Lower numbers stick more closely to the input image"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.LTX_Video

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Lipsync_2(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate realistic lipsyncs with Sync Labs' 2.0 model
    """

    Sync_mode: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Lipsync_2.Sync_mode
    )

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input audio file (.wav)",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video file (.mp4)",
    )
    sync_mode: nodetool.nodes.replicate.video.generate.Lipsync_2.Sync_mode = Field(
        default=nodetool.nodes.replicate.video.generate.Lipsync_2.Sync_mode("loop"),
        description="Lipsync mode when audio and video durations are out of sync",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.5, description="How expressive lipsync can be (0-1)"
    )
    active_speaker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to detect active speaker (i.e. whoever is speaking in the clip will be used for lipsync)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Lipsync_2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Lipsync_2_Pro(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Studio-grade lipsync in minutes, not weeks
    """

    Sync_mode: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Lipsync_2_Pro.Sync_mode
    )

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input audio file (.wav)",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video file (.mp4)",
    )
    sync_mode: nodetool.nodes.replicate.video.generate.Lipsync_2_Pro.Sync_mode = Field(
        default=nodetool.nodes.replicate.video.generate.Lipsync_2_Pro.Sync_mode("loop"),
        description="Lipsync mode when audio and video durations are out of sync",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.5, description="How expressive lipsync can be (0-1)"
    )
    active_speaker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to detect active speaker (i.e. whoever is speaking in the clip will be used for lipsync)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Lipsync_2_Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Music_01(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Quickly generate up to 1 minute of music with lyrics and vocals in the style of a reference track
    """

    Bitrate: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Music_01.Bitrate
    )
    Sample_rate: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Music_01.Sample_rate
    )

    lyrics: str | OutputHandle[str] = connect_field(
        default="",
        description="Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 350 to 400 characters.",
    )
    bitrate: nodetool.nodes.replicate.video.generate.Music_01.Bitrate = Field(
        default=nodetool.nodes.replicate.video.generate.Music_01.Bitrate(256000),
        description="Bitrate for the generated music",
    )
    voice_id: str | OutputHandle[str] | None = connect_field(
        default=None, description="Reuse a previously uploaded voice ID"
    )
    song_file: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference song, should contain music and vocals. Must be a .wav or .mp3 file longer than 15 seconds.",
    )
    voice_file: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Voice reference. Must be a .wav or .mp3 file longer than 15 seconds. If only a voice reference is given, an a cappella vocal hum will be generated.",
    )
    sample_rate: nodetool.nodes.replicate.video.generate.Music_01.Sample_rate = Field(
        default=nodetool.nodes.replicate.video.generate.Music_01.Sample_rate(44100),
        description="Sample rate for the generated music",
    )
    instrumental_id: str | OutputHandle[str] | None = connect_field(
        default=None, description="Reuse a previously uploaded instrumental ID"
    )
    instrumental_file: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Instrumental reference. Must be a .wav or .mp3 file longer than 15 seconds. If only an instrumental reference is given, a track without vocals will be generated.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Music_01

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Pixverse_V5(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Create 5s-8s videos with enhanced character movement, visual effects, and exclusive 1080p-8s support. Optimized for anime characters and complex actions
    """

    Effect: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Pixverse_V5.Effect
    )
    Quality: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Pixverse_V5.Quality
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Pixverse_V5.Duration
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Pixverse_V5.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: str | OutputHandle[str] | None = connect_field(
        default=None, description="Image to use for the first frame of the video"
    )
    effect: nodetool.nodes.replicate.video.generate.Pixverse_V5.Effect = Field(
        default=nodetool.nodes.replicate.video.generate.Pixverse_V5.Effect("None"),
        description="Special effect to apply to the video. V5 supports effects. Does not work with last_frame_image.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for video generation"
    )
    quality: nodetool.nodes.replicate.video.generate.Pixverse_V5.Quality = Field(
        default=nodetool.nodes.replicate.video.generate.Pixverse_V5.Quality("540p"),
        description="Resolution of the video. 360p and 540p cost the same, but 720p and 1080p cost more. V5 supports 1080p with 8 second duration.",
    )
    duration: nodetool.nodes.replicate.video.generate.Pixverse_V5.Duration = Field(
        default=nodetool.nodes.replicate.video.generate.Pixverse_V5.Duration(5),
        description="Duration of the video in seconds. 8 second videos cost twice as much as 5 second videos. V5 supports 1080p with 8 second duration.",
    )
    aspect_ratio: nodetool.nodes.replicate.video.generate.Pixverse_V5.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.video.generate.Pixverse_V5.Aspect_ratio(
                "16:9"
            ),
            description="Aspect ratio of the video",
        )
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to avoid certain elements in the video"
    )
    last_frame_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Use to generate a video that transitions from the first image to the last image. Must be used with image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Pixverse_V5

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Ray(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Fast, high quality text-to-video and image-to-video (Also known as Dream Machine)
    """

    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Ray.Aspect_ratio
    )

    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop, with the last frame matching the first frame for smooth, continuous playback. This input is ignored if end_image_url or end_video_id are set.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for video generation"
    )
    end_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="An optional last frame of the video to use as the ending frame.",
    )
    start_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="An optional first frame of the video to use as the starting frame.",
    )
    aspect_ratio: nodetool.nodes.replicate.video.generate.Ray.Aspect_ratio = Field(
        default=nodetool.nodes.replicate.video.generate.Ray.Aspect_ratio("16:9"),
        description="Aspect ratio of the video. Ignored if a start frame, end frame or video ID is given.",
    )
    end_video_id: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Prepend a new video generation to the beginning of an existing one (Also called 'reverse extend'). You can combine this with start_image_url, or start_video_id.",
    )
    end_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of an image to use as the ending frame",
    )
    start_video_id: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Continue or extend a video generation with a new generation. You can combine this with end_image_url, or end_video_id.",
    )
    start_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of an image to use as the starting frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Ray

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class RobustVideoMatting(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    extract foreground of a video
    """

    Output_type: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.RobustVideoMatting.Output_type
    )

    input_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Video to segment.",
    )
    output_type: (
        nodetool.nodes.replicate.video.generate.RobustVideoMatting.Output_type
    ) = Field(
        default=nodetool.nodes.replicate.video.generate.RobustVideoMatting.Output_type(
            "green-screen"
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.RobustVideoMatting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Video_01(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate 6s videos with prompts or images. (Also known as Hailuo). Use a subject reference to make a video with a character and the S2V-01 model.
    """

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for generation"
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use prompt optimizer"
    )
    first_frame_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="First frame image for video generation. The output video will have the same aspect ratio as this image.",
    )
    subject_reference: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="An optional character reference image to use as the subject in the generated video (this will use the S2V-01 model)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Video_01

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Video_01_Live(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    An image-to-video (I2V) model specifically trained for Live2D and general animation use cases
    """

    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for generation"
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use prompt optimizer"
    )
    first_frame_image: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="First frame image for video generation. The output video will have the same aspect ratio as this image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Video_01_Live

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Wan_2_1_1_3B(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate 5s 480p videos. Wan is an advanced and powerful visual generation model developed by Tongyi Lab of Alibaba Group
    """

    Frame_num: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Frame_num
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Resolution
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None,
        description="Random seed for reproducible results (leave blank for random)",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt describing what you want to generate"
    )
    frame_num: nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Frame_num = Field(
        default=nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Frame_num(81),
        description="Video duration in frames (based on standard 16fps playback)",
    )
    resolution: nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Resolution = Field(
        default=nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Resolution("480p"),
        description="Video resolution",
    )
    aspect_ratio: nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Aspect_ratio = (
        Field(
            default=nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B.Aspect_ratio(
                "16:9"
            ),
            description="Video aspect ratio",
        )
    )
    sample_shift: float | OutputHandle[float] = connect_field(
        default=8,
        description="Sampling shift factor for flow matching (recommended range: 8-12)",
    )
    sample_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of sampling steps (higher = better quality but slower)",
    )
    sample_guide_scale: float | OutputHandle[float] = connect_field(
        default=6,
        description="Classifier free guidance scale (higher values strengthen prompt adherence)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Wan_2_1_1_3B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Wan_2_1_I2V_480p(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Accelerated inference for Wan 2.1 14B image to video, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation.
    """

    Fast_mode: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_1_I2V_480p.Fast_mode
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_1_I2V_480p.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Set for reproducible generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image for use as the initial frame of the video.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Text prompt for image generation"
    )
    fast_mode: nodetool.nodes.replicate.video.generate.Wan_2_1_I2V_480p.Fast_mode = (
        Field(
            default=nodetool.nodes.replicate.video.generate.Wan_2_1_I2V_480p.Fast_mode(
                "Balanced"
            ),
            description="Speed up generation with different levels of acceleration. Faster modes may degrade quality somewhat. The speedup is dependent on the content, so different videos may see different speedups.",
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Determines how strongly the main LoRA should be applied. Sane results between 0 and 1 for base inference. You may still need to experiment to find the best value for your particular lora.",
    )
    aspect_ratio: (
        nodetool.nodes.replicate.video.generate.Wan_2_1_I2V_480p.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.video.generate.Wan_2_1_I2V_480p.Aspect_ratio(
            "16:9"
        ),
        description="Aspect ratio of the output video.",
    )
    lora_weights: str | OutputHandle[str] | None = connect_field(
        default=None,
        description="Load LoRA weights. Supports HuggingFace URLs in the format huggingface.co/<owner>/<model-name>, CivitAI URLs in the format civitai.com/models/<id>[/<model-name>], or arbitrary .safetensors URLs from the Internet.",
    )
    sample_shift: int | OutputHandle[int] = connect_field(
        default=3, description="Flow shift parameter for video generation"
    )
    sample_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to avoid certain elements"
    )
    sample_guide_scale: float | OutputHandle[float] = connect_field(
        default=5, description="Guidance scale for generation"
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated videos"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Wan_2_1_I2V_480p

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Wan_2_2_I2V_Fast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    A very fast and cheap PrunaAI optimized version of Wan 2.2 A14B image-to-video
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_2_I2V_Fast.Resolution
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_2_I2V_Fast.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank for random"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to generate video from.",
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for video generation"
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True, description="Go fast"
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of video frames. 81 frames give the best results",
    )
    resolution: nodetool.nodes.replicate.video.generate.Wan_2_2_I2V_Fast.Resolution = (
        Field(
            default=nodetool.nodes.replicate.video.generate.Wan_2_2_I2V_Fast.Resolution(
                "720p"
            ),
            description="Resolution of video. 16:9 corresponds to 832x480px, and 9:16 is 480x832px",
        )
    )
    aspect_ratio: (
        nodetool.nodes.replicate.video.generate.Wan_2_2_I2V_Fast.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.video.generate.Wan_2_2_I2V_Fast.Aspect_ratio(
            "16:9"
        ),
        description="Aspect ratio of video. 16:9 corresponds to 832x480px, and 9:16 is 480x832px",
    )
    sample_shift: float | OutputHandle[float] = connect_field(
        default=12, description="Sample shift factor"
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second. Note that the pricing of this model is based on the video duration at 16 fps",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Wan_2_2_I2V_Fast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Wan_2_2_T2V_Fast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    A very fast and cheap PrunaAI optimized version of Wan 2.2 A14B text-to-video
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_2_T2V_Fast.Resolution
    )
    Aspect_ratio: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Wan_2_2_T2V_Fast.Aspect_ratio
    )

    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank for random"
    )
    prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Prompt for video generation"
    )
    go_fast: bool | OutputHandle[bool] = connect_field(
        default=True, description="Go fast"
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of video frames. 81 frames give the best results",
    )
    resolution: nodetool.nodes.replicate.video.generate.Wan_2_2_T2V_Fast.Resolution = (
        Field(
            default=nodetool.nodes.replicate.video.generate.Wan_2_2_T2V_Fast.Resolution(
                "720p"
            ),
            description="Resolution of video. 16:9 corresponds to 832x480px, and 9:16 is 480x832px",
        )
    )
    aspect_ratio: (
        nodetool.nodes.replicate.video.generate.Wan_2_2_T2V_Fast.Aspect_ratio
    ) = Field(
        default=nodetool.nodes.replicate.video.generate.Wan_2_2_T2V_Fast.Aspect_ratio(
            "16:9"
        ),
        description="Aspect ratio of video. 16:9 corresponds to 832x480px, and 9:16 is 480x832px",
    )
    sample_shift: float | OutputHandle[float] = connect_field(
        default=12, description="Sample shift factor"
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second. Note that the pricing of this model is based on the video duration at 16 fps",
    )
    disable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="Disable safety checker for generated video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Wan_2_2_T2V_Fast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.replicate.video.generate
from nodetool.workflows.base_node import BaseNode


class Zeroscope_V2_XL(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Zeroscope V2 XL & 576w
    """

    Model: typing.ClassVar[type] = (
        nodetool.nodes.replicate.video.generate.Zeroscope_V2_XL.Model
    )

    fps: int | OutputHandle[int] = connect_field(
        default=8, description="fps for the output video"
    )
    seed: int | OutputHandle[int] | None = connect_field(
        default=None, description="Random seed. Leave blank to randomize the seed"
    )
    model: nodetool.nodes.replicate.video.generate.Zeroscope_V2_XL.Model = Field(
        default=nodetool.nodes.replicate.video.generate.Zeroscope_V2_XL.Model("xl"),
        description="Model to use",
    )
    width: int | OutputHandle[int] = connect_field(
        default=576, description="Width of the output video"
    )
    height: int | OutputHandle[int] = connect_field(
        default=320, description="Height of the output video"
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="An astronaut riding a horse", description="Input prompt"
    )
    batch_size: int | OutputHandle[int] = connect_field(
        default=1, description="Batch size"
    )
    init_video: str | OutputHandle[str] | None = connect_field(
        default=None, description="URL of the initial video (optional)"
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=24, description="Number of frames for the output video"
    )
    init_weight: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of init_video"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale"
    )
    negative_prompt: str | OutputHandle[str] | None = connect_field(
        default=None, description="Negative prompt"
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=False, description="Remove watermark"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.replicate.video.generate.Zeroscope_V2_XL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
